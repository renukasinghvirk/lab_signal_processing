{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a59cf561",
   "metadata": {},
   "source": [
    "**Students' name:**\n",
    "\n",
    "**Camille Pittet (326866), Renuka Singh Virk (326470)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb65de",
   "metadata": {},
   "source": [
    "This aim of this course is to review the evolution of image processing tools, from basics to deep learning algorithms. The semester is split into four labs :\n",
    "\n",
    "* **Lab 1** : Introduction to Image Processing\n",
    "* **Lab 2** : Object detection\n",
    "* **Lab 3** : Object tracking\n",
    "* **Lab 4** : Introduction to Deep Learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9952878",
   "metadata": {},
   "source": [
    "Let's start with the first chapter of this course!\n",
    "\n",
    "# Chapter 1 : Introduction to Image Processing\n",
    "\n",
    "(100 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99be909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b5fd3",
   "metadata": {},
   "source": [
    "## 1.1 Introduction to Basic Image Processing Using OpenCV and NumPy\n",
    "\n",
    "(20 points)\n",
    "\n",
    "In this section we start with the basic image processing in Python.\n",
    "\n",
    "**Goals:**\n",
    "\n",
    "1. Read an image, access and modify pixel values\n",
    "2. Change color space\n",
    "3. Understand the concepts of histogram and histogram equalization\n",
    "\n",
    "\n",
    "### 1.1.1 Read an image, access and modify pixel values\n",
    "\n",
    "The code in the next section shows how you can load an image into the Jupyter notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec27e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reading an image\n",
    "img = cv.imread(os.path.join('.', 'data', 'matterhorn.jpg'))\n",
    "print('Image has dimensions: {}'.format(img.shape))\n",
    "print('There is a total of {} elements'.format(img.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f32870",
   "metadata": {},
   "source": [
    "OpenCV loads the image in a type of variable called a NumPy array. Elements of a NumPy array are accessed through indexing with the operator `[.]` where `.` indicates which dimensions you're accessing. The dimensions are organized as follows: `rows, cols, channels`. More information about indexing in NumPy is in the [doc](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d26d33",
   "metadata": {},
   "source": [
    "**QUESTION:** (/2)\n",
    "- What do `rows, cols, channels` represent in the image? \n",
    "- What is the value of channels for grayscale and colored image? \n",
    "- Is the `matterhorn.jpg` image a grayscale or color image? Why? \n",
    "- What is the meaning of each channel when the image is loaded with OpenCV ([Hint](https://docs.opencv.org/4.x/d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada6c107",
   "metadata": {},
   "source": [
    "**YOUR ANSWER** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2c532d",
   "metadata": {},
   "source": [
    "In the next cell we display the image using the provided function `display_image()` in file `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2e8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image can be displayed like this\n",
    "display_image(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa71cbb5",
   "metadata": {},
   "source": [
    "Now that you have an image loaded in the notebook, let's extract some properties from it. Here is how you can access a pixel or a region of the image, and modify a pixel value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4976248",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_row = 10 \n",
    "pixel_col = 10\n",
    "\n",
    "# accessing pixel (pixel_x, pixel_y) \n",
    "print('Pixel value at location {}x{} is {}'.format(pixel_row, pixel_col, img[pixel_row, pixel_col]))\n",
    "\n",
    "# accessing region of the image starting at(pixel_row, pixel_col)\n",
    "region_size = 10 \n",
    "img_region = img[pixel_row:pixel_row+region_size, pixel_col:pixel_col+region_size]\n",
    "print('Region size is {}'.format(img_region.shape))\n",
    "\n",
    "# modifying pixel value\n",
    "img[pixel_row, pixel_col] = [0, 0, 0]\n",
    "print('New pixel value at location {}x{} is {}'.format(pixel_row, pixel_col, img[pixel_row, pixel_col]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda73c5",
   "metadata": {},
   "source": [
    "**QUESTION:** (/1)\n",
    "\n",
    "In the next cell, change the color of the pixels to white in a rectangle of size (40, 100) starting from the pixel (30, 60) and show the new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf75811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying pixel value\n",
    "\n",
    "## YOUR CODE HERE\n",
    "\n",
    "# Display the modified image with white rectangle\n",
    "display_image(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae38358b",
   "metadata": {},
   "source": [
    "### 1.1.2 Colorspace Conversion\n",
    "\n",
    "Now, you will learn how to convert images from one colorspace to another, like BGR ↔ Gray, BGR ↔ YUV etc. There are more than 150 color-space conversion methods available in OpenCV. \n",
    "\n",
    "**QUESTION:** (/4)\n",
    "\n",
    "In the next two cells, implements the conversion function going from BGR image to GRAY image and BGR to YUV image. There are built-in functions in OpenCV doing exactly that but for the moment we'll ask you to implement your **own** function in order to get the idea of what is happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee4756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bgr_to_grayscale(frame):\n",
    "    \"\"\"\n",
    "    Convert a given bgr image into grayscale image\n",
    "    :param frame: Color image to convert\n",
    "    :return: Grayscale image as numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE   \n",
    "    \n",
    "    return image.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b406a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bgr_to_yuv(frame):\n",
    "    \"\"\"\n",
    "    Convert a given rgb image into hsv image\n",
    "    :param frame: Color image to convert\n",
    "    :return: YUV image as numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    return image.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965aca21",
   "metadata": {},
   "source": [
    "The following lines shows the conversion based on your implementation and the built-in function from OpenCV. If everything went well, the two should match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a622d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join('.', 'data', 'matterhorn.jpg'))\n",
    "# convert to gray + hsv\n",
    "img_gray = convert_bgr_to_grayscale(img)\n",
    "img_yuv = convert_bgr_to_yuv(img)\n",
    "gt_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "gt_yuv = cv.cvtColor(img, cv.COLOR_BGR2YUV)\n",
    "# Display\n",
    "fig, ax = plt.subplots(2, 3, figsize=(12, 7))\n",
    "display_image(img, axes=ax[0][0])\n",
    "ax[0][0].set_title('Original')\n",
    "display_image(img_gray, axes=ax[0][1])\n",
    "ax[0][1].set_title('Grayscale')\n",
    "display_image(img_yuv, axes=ax[0][2])\n",
    "ax[0][2].set_title('YUV')\n",
    "\n",
    "display_image(img, axes=ax[1][0])\n",
    "ax[1][0].set_title('Original')\n",
    "display_image(gt_gray, axes=ax[1][1])\n",
    "ax[1][1].set_title('OpenCV - Grayscale')\n",
    "display_image(gt_yuv, axes=ax[1][2])\n",
    "ax[1][2].set_title('OpenCV - YUV');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a3cd8",
   "metadata": {},
   "source": [
    "Now you know how to convert *BGR* image to any color space, you can use this to extract a colored object. In *HSV*, it is more easier to represent a color than in *BGR* colorspace. Here is the method to extract a colored object:\n",
    "\n",
    "1. Reading an input image\n",
    "2. Convert from *BGR* to *HSV* color-space using bluit-in function from OpenCV\n",
    "3. We threshold the *HSV* image to extract the pixels in the desired color range. \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "First, implement the function `threshold_bgr_image()` to threshold the color image in the desired color range. \n",
    "\n",
    "**Hint**: You may use the OpenCV functions `inRange()` and `bitwise_and()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_bgr_image(frame, lower, upper):\n",
    "    \"\"\"\n",
    "    Segment image.\n",
    "        1. Convert image to HSV color space\n",
    "        2. Find pixels in the desired color range (lower, upper)\n",
    "        3. Apply the mask to the image. \n",
    "    :param frame: BGR image to segment\n",
    "    :param lower: Lower threshold value\n",
    "    :param upper: Upper threshold value\n",
    "    :return res: Segmented image\n",
    "    :return mask: binary mask\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    return res, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632e4658",
   "metadata": {},
   "source": [
    "The next cell test your function on the blue pixels of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51333bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read input\n",
    "img = cv.imread(os.path.join('..','data', 'circles_color.png'))\n",
    "\n",
    "# 2. Define range of blue color in HSV\n",
    "lower = np.asarray([90,120,120])\n",
    "upper = np.asarray([130,255,255])\n",
    "im, mask = threshold_bgr_image(img, lower, upper)\n",
    "\n",
    "# 3. Display using subplots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(17, 9))\n",
    "display_image(img, axes=ax[0])\n",
    "ax[0].set_title('Input')\n",
    "display_image(mask, axes=ax[1], cmap='gray')\n",
    "ax[1].set_title('Detected Mask')\n",
    "display_image(im, axes=ax[2])\n",
    "ax[2].set_title('Extracted color');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd6101",
   "metadata": {},
   "source": [
    "In order to find *HSV* values, you can use the same function, `cv.cvtColor()`. Instead of passing an image, you just pass the *BGR* values you want. The next cell shows an example to find the HSV value of red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "red = np.uint8([[[0, 0, 255 ]]])\n",
    "hsv_red = cv.cvtColor(red, cv.COLOR_BGR2HSV)\n",
    "print( 'HSV component {}'.format(hsv_red))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46cc678",
   "metadata": {},
   "source": [
    "**QUESTION:** (/2)\n",
    "\n",
    "Try to extract the **Yellow** circle. Comment and show the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc69b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read input\n",
    "img = cv.imread(os.path.join('..','data', 'circles_color.png'))\n",
    "\n",
    "# 2. Extract yellow color in HSV\n",
    "\n",
    "### Your code here ### \n",
    "\n",
    "\n",
    "# 3. Display using subplots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(17, 9))\n",
    "display_image(img, axes=ax[0])\n",
    "ax[0].set_title('Input')\n",
    "display_image(mask, axes=ax[1], cmap='gray')\n",
    "ax[1].set_title('Detected Mask')\n",
    "display_image(im, axes=ax[2])\n",
    "ax[2].set_title('Extracted color')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e916ec",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8636022",
   "metadata": {},
   "source": [
    "So far you have been working with color image, but you could apply the same approach with the grayscale image.\n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "Now, try to extract the **yellow** pencil using the grayscale image. What are the pros and cons of each method (colors *vs* grayscale)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b26976",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def threshold_gray_image(frame, lower, upper):\n",
    "    \"\"\"\n",
    "    Segment image.\n",
    "        1. Convert image to gray color space\n",
    "        2. Find pixels in the desired color range (lower, upper)\n",
    "        3. Apply the mask to the image. \n",
    "    :param frame: BGR image to segment\n",
    "    :param lower: Lower threshold value\n",
    "    :param upper: Upper threshold value\n",
    "    :return: Segmented image, binary mask\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return res, mask\n",
    "\n",
    "# 1. Read input\n",
    "img = cv.imread(os.path.join('..','data', 'circles_color.png'))\n",
    "\n",
    "# 2. Extract yellow color in gray\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "# 3. Display using subplots\n",
    "fig, ax = plt.subplots(1, 3, figsize=(17, 9))\n",
    "display_image(img, axes=ax[0])\n",
    "ax[0].set_title('Input')\n",
    "display_image(mask, axes=ax[1], cmap='gray')\n",
    "ax[1].set_title('Detected Mask')\n",
    "display_image(im, axes=ax[2])\n",
    "ax[2].set_title('Extracted color')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a3394",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a951e1",
   "metadata": {},
   "source": [
    "### 1.1.3 Image Histogram\n",
    "\n",
    "An histogram is a graph or plot, which gives you an overall idea about the intensity distribution of the pixels of the image. It is just another way of understanding the image. By looking at the histogram of an image, you get intuition about contrast, brightness, intensity distribution etc of that image. Almost all image processing tools today, provides features on histogram.\n",
    "\n",
    "**QUESTION:** (/1)\n",
    "\n",
    "In the next cell, implement a function using OpenCV that computes the image histogram for all channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf1350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_histogram(image):\n",
    "    \"\"\"\n",
    "    Compute image color distribution for each channels of a given `image`\n",
    "    :param image: Image to extract histograms for\n",
    "    :return: List of histogram, one per channel.\n",
    "    \"\"\"\n",
    "    hist = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load image\n",
    "img = cv.imread(os.path.join('..','data', 'matterhorn.jpg'))\n",
    "\n",
    "# 2. Compute histogram for each channels\n",
    "hists = compute_image_histogram(img)\n",
    "\n",
    "# 3. Plot \n",
    "color = ['b', 'g', 'r']\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 7))\n",
    "display_image(img, axes=ax[0])\n",
    "for k, hist in enumerate(hists):\n",
    "    ax[1].plot(hist, color=color[k])\n",
    "ax[1].set_title('Color Histogram by channels')\n",
    "ax[1].set_xlim([0, 256])\n",
    "ax[1].set_xlabel('Pixel Intensity')\n",
    "ax[1].set_ylabel('Number of Pixel')\n",
    "plt.legend(['Blue Channel', 'Green Channel', 'Red Channel'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c6def",
   "metadata": {},
   "source": [
    "You can see the image and the corresponding histograms.\n",
    "\n",
    "**QUESTION:** (/1)\n",
    "\n",
    "What is the significant of the pixel intensity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c3b507",
   "metadata": {},
   "source": [
    "**YOUR ANSWER** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3eec22",
   "metadata": {},
   "source": [
    "#### Histogram equalization\n",
    "Consider an image whose pixel values are confined to some specific range of values only. For instance, brighter image will have all pixels confined to high values. But a good image will have pixels from all regions of the image. So you need to stretch this histogram to either ends and that is what Histogram Equalization does. This normally improves the contrast of the image ([doc](https://en.wikipedia.org/wiki/Histogram_equalization)).\n",
    "\n",
    "**QUESTION:** (/3)\n",
    "\n",
    "In the next section implement such contrast enhancement function (i.e. based on histogram equalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_enhancement(image):\n",
    "    \"\"\"\n",
    "    Compute histogram and apply histogram equalization on a given image.\n",
    "    :param image: Image to enhance contrast\n",
    "    :return: tuple: enhanced image, input image histogram, transformed image histogram\n",
    "    \"\"\"\n",
    "    \n",
    "    hist = None\n",
    "    hist_equ = None\n",
    "    image_equ = None\n",
    "    \n",
    "    # Apply histogram equalization\n",
    "    \n",
    "    return image_equ, hist, hist_equ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1122a845",
   "metadata": {},
   "source": [
    "The next cell shows you the results of histogram equalization on an image. \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "- How has the contrast evolved after applying the transformation ? \n",
    "- How has the histogram changed ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join('..','data', 'matterhorn_snow.jpg'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Enhance contrast with histogram equalization\n",
    "img_equ, hist, hist_equ = contrast_enhancement(img)\n",
    "\n",
    "# Display\n",
    "fig, ax = plt.subplots(2, 2, figsize=(17, 9))\n",
    "display_image(img, axes=ax[0][0])\n",
    "ax[0][0].set_title('Original')\n",
    "# Histogram original\n",
    "ax[0][1].bar(np.arange(hist.shape[0]), hist.reshape(-1),  width=1)\n",
    "ax[0][1].set_xlim([0, 256])\n",
    "ax[0][1].set_title('Histogram')\n",
    "# Histogram equalized\n",
    "display_image(img_equ, axes=ax[1][0])\n",
    "ax[1][0].set_title('Histogram Equalized')\n",
    "ax[1][1].bar(np.arange(hist_equ.shape[0]), hist_equ.reshape(-1),  width=1)\n",
    "ax[1][1].set_xlim([0, 256])\n",
    "ax[1][1].set_title('Histogram')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76afa51",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe04cb63",
   "metadata": {},
   "source": [
    "### 1.2 Image filtering and edges detection\n",
    "\n",
    "(50 points)\n",
    "\n",
    "Image filtering is a two-dimensional convolution of the image by a small matrix called kernel. The choice of the kernel depends on the goal of the filtering operation, which can be noise removal, edge detection, contrast enhancement, etc. In general, filters are classified into two families: low-pass filters and high-pass filters. \n",
    " \n",
    "### 1.2.1 Low-pass filters \n",
    "\n",
    "The most common low-pass filters are the [averaging filter](https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga8c45db9afe636703801b0b2e440fce37), the [Gaussian filter](https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1) and the [median filter](https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9). \n",
    "\n",
    "**QUESTION:** (/3)\n",
    "\n",
    "In the next cell, implement the function `apply_lowpass_filter()`, which takes as input an image, and the filter parameters in a dictionary (filter type and size) and returns the filtered image. Use the corresponding OpenCV functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b4863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lowpass_filter(img, filter_params):\n",
    "    \"\"\"\n",
    "    Apply a low-pass filter on a given image.\n",
    "    :param image: Image to enhance contrast\n",
    "    :param filter_params: Parameters of the filter\n",
    "    :return: img_filt: filtered image\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    return img_filt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28e370",
   "metadata": {},
   "source": [
    "The next cell tests your implementation on two images corrupted by a different noise, with a filter of size (7x7). \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "What is the effect of each filter? How different are the filtered images? Which filter is the best for each image and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5da46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "imgs = [cv.imread(os.path.join('..','data', 'matterhorn_noise1.jpg'), cv.IMREAD_GRAYSCALE), cv.imread(os.path.join('..','data', 'matterhorn_noise2.jpg'), cv.IMREAD_GRAYSCALE)]\n",
    "img_title = [\"Salt & Pepper\", \"Gaussian\"]\n",
    "\n",
    "# Filtering with a kernel of size 7x7\n",
    "filter_size = 7 \n",
    "filter_params = [ {\"ftype\":\"averaging\", \"fsize\":filter_size},\n",
    "                 {\"ftype\":\"gaussian\", \"fsize\":filter_size},\n",
    "                 {\"ftype\":\"median\", \"fsize\":filter_size}]\n",
    "\n",
    "\n",
    "# Display image\n",
    "fig, ax = plt.subplots(len(filter_params)+1, len(imgs), figsize=(20, 20))\n",
    "\n",
    "for curr_img_idx, curr_img in enumerate(imgs):\n",
    "    display_image(curr_img, axes=ax[0, curr_img_idx], cmap='gray')\n",
    "    \n",
    "    ax[0, curr_img_idx].set_title(img_title[curr_img_idx], fontsize=30)\n",
    "\n",
    "    for curr_filt_idx, curr_filt in enumerate(filter_params):        \n",
    "        img_filt = apply_lowpass_filter(curr_img, curr_filt)\n",
    "        display_image(img_filt, axes=ax[curr_filt_idx+1, curr_img_idx], cmap='gray')\n",
    "        ax[curr_filt_idx+1, curr_img_idx].set_title(curr_filt[\"ftype\"], fontsize=30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4cc322",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b592ac5",
   "metadata": {},
   "source": [
    "Now that you are able to filter an image, let's find the optimal kernel. In this question, we consider the optimal filter to be the one that produces a filtered image that is visually closest to the image without noise.\n",
    "\n",
    "**QUESTION:** (/4)\n",
    "\n",
    "For each image, propose an optimal kernel (type and size), justify your choice and show the filtered image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f4aa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load image\n",
    "img_orig = cv.imread(os.path.join('..','data', 'matterhorn.jpg'), cv.IMREAD_GRAYSCALE)\n",
    "imgs = [cv.imread(os.path.join('..','data', 'matterhorn_noise1.jpg'), cv.IMREAD_GRAYSCALE), cv.imread(os.path.join('..','data', 'matterhorn_noise2.jpg'), cv.IMREAD_GRAYSCALE)]\n",
    "img_title = [\"Salt & Pepper\", \"Gaussian\"]\n",
    "\n",
    "# Best filter parameters\n",
    "\n",
    "\n",
    "###################\n",
    "# YOUR CODE HERE\n",
    "best_filter_params_1 = {}\n",
    "best_filter_params_2 = {}\n",
    "###################\n",
    "\n",
    "\n",
    "# Filtering\n",
    "img1_best = apply_lowpass_filter(imgs[0], best_filter_params_1)\n",
    "img2_best = apply_lowpass_filter(imgs[1], best_filter_params_2)\n",
    "\n",
    "# Display image\n",
    "fig, ax = plt.subplots(1, 3, figsize=(30, 10))\n",
    "display_image(img_orig, axes=ax[0], cmap='gray')\n",
    "ax[0].set_title(\"Orignal image\", fontsize=30)\n",
    "display_image(img1_best, axes=ax[1], cmap='gray')\n",
    "ax[1].set_title(\"Best filter - S&P Noise\", fontsize=30)\n",
    "display_image(img2_best, axes=ax[2], cmap='gray')\n",
    "ax[2].set_title(\"Best filter - Gaussian noise\", fontsize=30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3f7c0",
   "metadata": {},
   "source": [
    "### 1.2.2 High-pass filters\n",
    "\n",
    "The second family of filters are the high-pass filters. You will try two of them: the first-order [Sobel filter](https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d) and the second order [Laplacian filter](https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#gad78703e4c8fe703d479c1860d76429e6).\n",
    "\n",
    "We start with the first-order Sobel filter, more info is available in the [doc](https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d). When using first-order filters, the kernel is applied in one direction, along the rows or the columns of the image. \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "In the next cell, implement the function `apply_sobel_filter` which takes as input an image, the\n",
    "direction of filtering (`x` or `y`), and the filter size  and returns the filtered image. Use the corresponding OpenCV function with the parameter `ddepth=cv.CV_64F`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab0db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sobel_filter(img, direction, filter_size):\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply the sobel filter on a given image.\n",
    "    :param img: Image to enhance contrast\n",
    "    :param direction: Direction of the derivative\n",
    "    :param filter_size: Size of the filter\n",
    "    :return: img_filt: filtered image\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE \n",
    "    \n",
    "    return img_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b465ac45",
   "metadata": {},
   "source": [
    "The next cell tests your implementation with a filter of size (7x7) in both directions. \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "What are the differences between the filtered images? Why? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join('..','data', 'matterhorn.jpg'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Sobel filtering\n",
    "filter_size = 7\n",
    "\n",
    "img_sobel_dx = apply_sobel_filter(img, \"x\", filter_size)\n",
    "img_sobel_dy = apply_sobel_filter(img, \"y\", filter_size)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "display_image(img_sobel_dx, axes=ax[0], cmap='gray')\n",
    "ax[0].set_title(\"Direction - dx\", fontsize=30)\n",
    "display_image(img_sobel_dy, axes=ax[1], cmap='gray')\n",
    "ax[1].set_title(\"Direction - dy\", fontsize=30)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae47e6bb",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b58189",
   "metadata": {},
   "source": [
    "Now, you will implement a second-order high-pass filter, the [Laplacian filter](https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#gad78703e4c8fe703d479c1860d76429e6). \n",
    "\n",
    "**QUESTION:** (/1)\n",
    "\n",
    "In the next cell, implement the function `apply_laplacian_filter` which takes as input an image, and the filter size and returns the filtered image. Use the corresponding OpenCV function with the parameter `ddepth=cv.CV_64F`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ab7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_laplacian_filter(img, filter_size):\n",
    "    \"\"\"\n",
    "    Apply a laplace filter on a given image.\n",
    "    :param img: Image to enhance contrast\n",
    "    :param filter_size: Size of the filter\n",
    "    :return: img_filt: filtered image\n",
    "    \"\"\"\n",
    "        \n",
    "    # Laplacian filtering\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return img_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d4a130",
   "metadata": {},
   "source": [
    "The next cell tests your implementation with filters of size (1x1), (5x5) and (11x11). \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "What are the differences between the filtered images? Why? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08354d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join('..','data', 'matterhorn.jpg'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Laplacian filtering\n",
    "filter_size = [1, 5, 11]\n",
    "\n",
    "fig, ax = plt.subplots(1, len(filter_size), figsize=(10*len(filter_size), 10))\n",
    "\n",
    "for curr_filt_idx, curr_filt_size in enumerate(filter_size):\n",
    "    img_laplacian = apply_laplacian_filter(img, curr_filt_size)\n",
    "    display_image(img_laplacian, axes=ax[curr_filt_idx], cmap='gray')\n",
    "    ax[curr_filt_idx].set_title(\"Filter size : {}\".format(curr_filt_size), fontsize=30)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f8688",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe85b56",
   "metadata": {},
   "source": [
    "It is convenient to have one function for applying a high-pass filter, similarly to the function `apply_lowpass_filter()`. \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "In the next cell, implement the function `apply_highpass_filter()`, which takes as input an image and a dictionary of the filter parameters, and returns the filtered image. Use the functions you implemented in the previous cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d74020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_highpass_filter(img, filter_params):\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply a highpass filter on a given image.\n",
    "    :param image: Image to enhance contrast\n",
    "    :param filter_params: Parameters of the filter\n",
    "    :return: img_filt: filtered image\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    return img_filt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e58ae4",
   "metadata": {},
   "source": [
    "The next cell tests your implementation with an example of each high-pass filter. \n",
    "\n",
    "**QUESTION** (/3)\n",
    "\n",
    "Comment the differences between the images. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join('..','data', 'matterhorn.jpg'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "filter_size = 3\n",
    "\n",
    "# Sobel filtering\n",
    "sobel_params = {\"ftype\":\"sobel\", \"direction\": \"x\", \"fsize\": filter_size}\n",
    "\n",
    "# Laplacian filtering \n",
    "laplacian_params = {\"ftype\":\"laplacian\", \"fsize\": filter_size}\n",
    "\n",
    "highpass_params = [sobel_params, laplacian_params]\n",
    "\n",
    "fig, ax = plt.subplots(1, len(highpass_params), figsize=(10*len(highpass_params), 10))\n",
    "\n",
    "for curr_highpass_idx, curr_highpass_params in enumerate(highpass_params):\n",
    "    img_filt = apply_highpass_filter(img, curr_highpass_params)\n",
    "    display_image(img_filt, axes=ax[curr_highpass_idx], cmap='gray')\n",
    "    ax[curr_highpass_idx].set_title(\" Filter: {}\".format(curr_highpass_params[\"ftype\"]), fontsize=30)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9497a17",
   "metadata": {},
   "source": [
    "### 1.2.3 Image Thresholding\n",
    "\n",
    "Image tresholding consist in assigning a new value to a pixel given a threhsold. If the pixel value is greater than the threshold value, it is assigned one value (*maybe white*), else it is assigned another value (*maybe black*). The function used is `cv.threshold(<image>, <thresh>, <maxVal>, <type>)`. First argument is the source image, which should be a grayscale image. Second argument is the threshold value which is used to classify the pixel values. Third argument is the maxVal which represents the value to be given if pixel value is more than (sometimes less than) the threshold value. OpenCV provides different styles of thresholding and it is decided by the fourth parameter of the function. Different types are:\n",
    "\n",
    "1. `cv.THRESH_BINARY`\n",
    "2. `cv.THRESH_BINARY_INV`\n",
    "3. `cv.THRES_TRUNC`\n",
    "4. `cv.THRESH_TOZERO`\n",
    "5. `cv.THRESH_TOZERO_INV`\n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "In the next cell, implement the thresholding operations within the function `threshold_image()`. It takes as input a grayscale image and a dictionary of parameters for the OpenCV function `cv.threshold(<image>, <thresh>, <maxVal>, <type>)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71447f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_image(img, thresh_params):\n",
    "    \"\"\"\n",
    "    Apply different type of threshold to a given image + threshold value\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Apply a threshold on a given image.\n",
    "    :param image: Image to enhance contrast\n",
    "    :param thresh_params: Parameters of the thresholdinf\n",
    "    :return: img_thresh: thresholded image\n",
    "    \"\"\"    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return img_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c53ecb",
   "metadata": {},
   "source": [
    "The next cell tests your implementation. \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join('..','data', 'grad.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Various threshold output\n",
    "thresh  = 127 \n",
    "tvalue  = 255\n",
    "\n",
    "thresh_type = [cv.THRESH_BINARY, cv.THRESH_BINARY_INV, cv.THRESH_TRUNC, cv.THRESH_TOZERO, cv.THRESH_TOZERO_INV]\n",
    "\n",
    "thresh_titles = ['BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(17, 9))\n",
    "display_image(img, axes=ax[0][0], cmap='gray')\n",
    "ax[0][0].set_title(\"Original image\")\n",
    "\n",
    "for curr_thresh_idx, curr_tthresh in enumerate(thresh_type):\n",
    "\n",
    "    thresh_params = {\"ttype\": curr_tthresh, 'thresh':thresh, 'value': tvalue}\n",
    "    thresh_img = threshold_image(img, thresh_params)\n",
    "\n",
    "    # Draw each samples\n",
    "    k=curr_thresh_idx+1\n",
    "    r = int(k / 3)\n",
    "    c = k % 3\n",
    "    display_image(thresh_img, axes=ax[r][c], cmap='gray')\n",
    "    ax[r][c].set_title(thresh_titles[curr_thresh_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922522be",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685614b1",
   "metadata": {},
   "source": [
    "\n",
    "#### Otsu Binarization\n",
    "\n",
    "In global thresholding, we used an arbitrary value for threshold value. How can we know a value we selected is good or not? Answer is, trial and error method. But consider an image whose histogram has two peaks (a bimodal image). For that image, we can approximately take a value in the middle of those peaks as threshold value. That is what Otsu binarization does. In simple words, it automatically calculates a threshold value from image histogram for a bimodal image. (For images which are not bimodal, binarization won’t be accurate.)\n",
    "\n",
    "For this, our `cv.threshold()` function is used, but pass an extra flag, `cv.THRES_OTSU`. For threshold value, simply pass zero. Then the algorithm finds the optimal threshold value and returns you as the second output, retVal. If Otsu thresholding is not used, retVal is same as the threshold value you used.\n",
    "\n",
    "\n",
    "Check out below example. Input image is a noisy image. In first case, a global thresholding value of 127 is applied. In second case, Otsu’s thresholding is applied directly. In third case, the image is filtered with a 5x5 gaussian kernel first to remove the noise, then applied Otsu thresholding. See how noise filtering improves the result.\n",
    "\n",
    "**QUESTIONS:** (/5)\n",
    "- For the global thresholding (case 1), why did we chose a value of 127? What would be the effect of a smaller threshold ?  Whats is the value of Otsu thresholding?\n",
    "- What would be the effect of a bigger Gaussian filter, for example 9x9 pixels? \n",
    "- In this example, Otsu's thresholding gives very good results. Can you describe an example for which this method fails? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac1b141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join('..','data', 'noisy2.png'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "tvalue = 255\n",
    "\n",
    "# 1. Global thresholding\n",
    "thresh_params = {\"ttype\": cv.THRESH_BINARY, 'thresh':127, 'value': tvalue}\n",
    "img_tglobal = threshold_image(img, thresh_params)\n",
    "\n",
    "# 2. Otsu thresholding\n",
    "thresh_otsu_params = {\"ttype\": cv.THRESH_BINARY+cv.THRESH_OTSU, 'thresh':0, 'value': tvalue}\n",
    "img_totsu = threshold_image(img, thresh_otsu_params)\n",
    "\n",
    "# 3. Gaussian blurring and Otsu thresholding\n",
    "thresh_params = {\"ttype\": cv.THRESH_BINARY+cv.THRESH_OTSU, 'thresh':0, 'value': tvalue}\n",
    "blur = cv.GaussianBlur(img, (5,5), 0)\n",
    "img_blur_totsu = threshold_image(blur, thresh_params)\n",
    "\n",
    "# Compute histograms\n",
    "hist_g = cv.calcHist([img], channels=[0], mask=None, histSize=[256], ranges=[0, 256])\n",
    "hist_f = cv.calcHist([blur], channels=[0], mask=None, histSize=[256], ranges=[0, 256])\n",
    "\n",
    "# plot all the images and their histograms\n",
    "images = [img, hist_g, img_tglobal,\n",
    "          img, hist_g, img_totsu,\n",
    "          blur, hist_f, img_blur_totsu]\n",
    "titles = ['Original Noisy Image', 'Histogram', 'Global Thresholding (v=127)',\n",
    "          'Original Noisy Image', 'Histogram', \"Otsu's Thresholding\",\n",
    "          'Gaussian filtered Image', 'Histogram', \"Otsu's Thresholding\"]\n",
    "\n",
    "# Display\n",
    "fig, ax = plt.subplots(3, 3, figsize=(17, 9))\n",
    "for k in range(9):\n",
    "    r = int(k / 3)\n",
    "    c = k % 3\n",
    "    \n",
    "    if c != 1:\n",
    "        # Show image\n",
    "        display_image(images[k], axes=ax[r][c], cmap='gray')\n",
    "        ax[r][c].set_title(titles[k])\n",
    "    else:\n",
    "        # Draw histogram\n",
    "        ax[r][c].bar(np.arange(images[k].shape[0]), images[k].reshape(-1), width=1)\n",
    "        ax[r][c].set_title(titles[k])\n",
    "        ax[r][c].set_xlabel('Pixel value')\n",
    "        ax[r][c].set_ylabel('Pixel count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf486529",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423e96b1",
   "metadata": {},
   "source": [
    "### 1.2.4 Edge detection and denoising\n",
    "\n",
    "Edge detection is an application of filtering, which consists in detecting the contours of the objects in the image. \n",
    "In this application, a low-pass filter, a high-pass filter and a binary thresholding operation are often used consecutively. \n",
    "\n",
    "We start with a naive approach to detect edges, a combination of the Sobel high-pass filter and a binary thresholding.  \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "In the next cell, implement the function `find_edges_naive()` with the following operations:\n",
    "- Use the function `apply_highpass_filter()` \n",
    "- Take the absolute value of the filtered image\n",
    "- Convert it to a uint8-grayscale image\n",
    "- Use the function `threshold_image()`on this grayscale image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b39cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_edges_naive(img, highpass_params, thresh_params):\n",
    "    \n",
    "    \"\"\"\n",
    "    Find edges on a given image.\n",
    "    :param image: Image to enhance contrast\n",
    "    :param highpass_params: Parameters of the high-pass filter\n",
    "    :param thresh_params: Parameters of the thresholding operation\n",
    "    :return: img_edges: Edges of the image\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return img_edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad30697",
   "metadata": {},
   "source": [
    "The next cell tests your implementation with a Sobel filter of size (3x3) in the x-direction and a binary threshold.\n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e862a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load image\n",
    "imgs = [cv.imread(os.path.join('..','data', 'matterhorn.jpg'), cv.IMREAD_GRAYSCALE), \n",
    "        cv.imread(os.path.join('..','data', 'matterhorn_noise1.jpg'), cv.IMREAD_GRAYSCALE),\n",
    "        cv.imread(os.path.join('..','data', 'matterhorn_noise2.jpg'), cv.IMREAD_GRAYSCALE)]\n",
    "\n",
    "imgs_name = [\"Noiseless\", \"S&P Noise\", \"Gaussian Noise\"]\n",
    "        \n",
    "filter_size = 3\n",
    "thresh = 50 \n",
    "tvalue = 255\n",
    "\n",
    "highpass_params = {\"ftype\":\"sobel\", \"direction\":\"x\", \"fsize\": filter_size}\n",
    "thresh_params = {\"ttype\": cv.THRESH_BINARY, 'thresh':thresh, 'value': tvalue}\n",
    "\n",
    "fig, ax = plt.subplots(len(imgs), 2, figsize=(20, 10*len(imgs)))\n",
    "\n",
    "for curr_img_idx, curr_img in enumerate(imgs):\n",
    "    img_edges = find_edges_naive(curr_img, highpass_params, thresh_params)\n",
    "\n",
    "    display_image(curr_img, axes=ax[curr_img_idx, 0], cmap='gray')\n",
    "    ax[curr_img_idx, 0].set_title(imgs_name[curr_img_idx], fontsize=30)\n",
    "    display_image(img_edges, axes=ax[curr_img_idx, 1], cmap='gray')\n",
    "    ax[curr_img_idx, 1].set_title(\"Edges\", fontsize=30)\n",
    "    \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f060dd87",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa1c3b",
   "metadata": {},
   "source": [
    "Now, let's improve this edge detector by adding a low-pass filter before using the function `find_edges_naive()`. \n",
    "\n",
    "\n",
    "\n",
    "**QUESTION:** (/1)\n",
    "\n",
    "In the next cell, code the function `find_edges_better()` with the following operations:\n",
    "- Use the function `apply_lowpass_filter()` \n",
    "- Use the function `find_edges_naive()` on the low-pass filtered image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee405ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_edges_better(img, lowpass_params, highpass_params, thresh_params):\n",
    "    \n",
    "    \"\"\"\n",
    "    Find edges on a given image.\n",
    "    :param image: Image to enhance contrast\n",
    "    :param lowpass_params: Parameters of the low-pass filter\n",
    "    :param highpass_params: Parameters of the high-pass filter\n",
    "    :param thresh_params: Parameters of the thresholding operation\n",
    "    :return: img_edges: Edges of the image\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return img_edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b7f039",
   "metadata": {},
   "source": [
    "The next cell tests your implementation with : \n",
    "- a Gaussian filter of size (3x3)\n",
    "- a Sobel filter of size (3x3) in the x-direction \n",
    "- a binary threshold.\n",
    "\n",
    "**QUESTION:** (/3)\n",
    "\n",
    "Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611456d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load image\n",
    "imgs = [cv.imread(os.path.join('..','data', 'matterhorn.jpg'), cv.IMREAD_GRAYSCALE), \n",
    "        cv.imread(os.path.join('..','data', 'matterhorn_noise1.jpg'), cv.IMREAD_GRAYSCALE),\n",
    "        cv.imread(os.path.join('..','data', 'matterhorn_noise2.jpg'), cv.IMREAD_GRAYSCALE)]\n",
    "\n",
    "imgs_name = [\"Noiseless\", \"S&P Noise\", \"Gaussian Noise\"]\n",
    "        \n",
    "lowpass_filter_size = 7\n",
    "highpass_filter_size = 3\n",
    "thresh = 50 \n",
    "tvalue = 255\n",
    "\n",
    "lowpass_params = {\"ftype\":\"gaussian\", \"fsize\": lowpass_filter_size}\n",
    "highpass_params = {\"ftype\":\"sobel\", \"direction\":\"x\", \"fsize\": highpass_filter_size}\n",
    "thresh_params = {\"ttype\": cv.THRESH_BINARY, 'thresh':thresh, 'value': tvalue}\n",
    "\n",
    "fig, ax = plt.subplots(len(imgs), 2, figsize=(20, 10*len(imgs)))\n",
    "\n",
    "for curr_img_idx, curr_img in enumerate(imgs):\n",
    "    img_edges = find_edges_better(curr_img, lowpass_params, highpass_params, thresh_params)\n",
    "\n",
    "    display_image(curr_img, axes=ax[curr_img_idx, 0], cmap='gray')\n",
    "    ax[curr_img_idx, 0].set_title(imgs_name[curr_img_idx], fontsize=30)\n",
    "    display_image(img_edges, axes=ax[curr_img_idx, 1], cmap='gray')\n",
    "    ax[curr_img_idx, 1].set_title(\"Edges\", fontsize=30)\n",
    "    \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4983e1e2",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a85705",
   "metadata": {},
   "source": [
    "Now that you developed some intuition, it is your turn to design the best edge detector for both noisy images. \n",
    "\n",
    "**QUESTION:** (/4)\n",
    "\n",
    "In the next cell, find the combination of low-pass filter, high-pass filter and thresholding operation that gives you the best edges for each noisy image. Justify your choice and comment your results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "imgs = [cv.imread(os.path.join('..','data', 'matterhorn_noise1.jpg'), cv.IMREAD_GRAYSCALE),\n",
    "        cv.imread(os.path.join('..','data', 'matterhorn_noise2.jpg'), cv.IMREAD_GRAYSCALE)]\n",
    "\n",
    "imgs_name = [\"S&P Noise\", \"Gaussian Noise\"]\n",
    "\n",
    "# Best detector parameters \n",
    "\n",
    "###################\n",
    "# YOUR CODE HERE\n",
    "#Gaussian image\n",
    "lowpass_params_g = {}\n",
    "highpass_params_g = {}\n",
    "thresh_params_g = {}\n",
    "\n",
    "#S&P image\n",
    "lowpass_params_sp = {}\n",
    "highpass_params_sp = {}\n",
    "thresh_params_sp = {}\n",
    "###################\n",
    "\n",
    "# Edge detection\n",
    "lowpass_params = [lowpass_params_sp, lowpass_params_g]\n",
    "highpass_params = [highpass_params_sp, highpass_params_g]\n",
    "thresh_params = [thresh_params_sp, thresh_params_g]\n",
    "\n",
    "fig, ax = plt.subplots(len(imgs), 2, figsize=(20, 10*len(imgs)))\n",
    "\n",
    "for curr_img_idx, curr_img in enumerate(imgs):\n",
    "    img_edges = find_edges_better(curr_img, lowpass_params[curr_img_idx], highpass_params[curr_img_idx], thresh_params[curr_img_idx])\n",
    "\n",
    "    display_image(curr_img, axes=ax[curr_img_idx, 0], cmap='gray')\n",
    "    ax[curr_img_idx, 0].set_title(imgs_name[curr_img_idx], fontsize=30)\n",
    "    display_image(img_edges, axes=ax[curr_img_idx, 1], cmap='gray')\n",
    "    ax[curr_img_idx, 1].set_title(\"{} - Edges\".format(imgs_name[curr_img_idx]), fontsize=30)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dfd01e",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8593a1c",
   "metadata": {},
   "source": [
    "As you can imagine, the OpenCV library has some edge detection algorithm directly available. One of them is the [Canny edge detector](https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga2a671611e104c093843d7b7fc46d24af). More info [here] (https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html). \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "In the next cell, implement the function `find_edges_canny()` which takes as input an image and the detector parameters (thresholds for filtering (See [doc](https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga2a671611e104c093843d7b7fc46d24af)), and the filter size) and returns the filtered image. Use the corresponding OpenCV function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd8fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_edges_canny(img, canny_params):\n",
    "    \"\"\"\n",
    "    Apply a canny filter on a given image.\n",
    "    :param img: Image to enhance contrast\n",
    "    :param th: Range of the filter threshold \n",
    "    :param filter_size: Size of the filter\n",
    "    :return: img_filt: filtered image\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return img_filt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a04836",
   "metadata": {},
   "source": [
    "The next cell tests your implementation with a filter of size (3x3) and four threshold ranges. \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "What is the effect of each threshold (low/high minimal threshold, low/high maximal threshold) on the filtered image?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = cv.imread(os.path.join('..','data', 'matterhorn.jpg'), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Canny filtering\n",
    "filter_size = 3\n",
    "\n",
    "thresholds = [(0, 10), (0, 600), (10, 300), (250, 300)]\n",
    "\n",
    "fig, ax = plt.subplots(1, len(thresholds), figsize=(10*len(thresholds), 10))\n",
    "\n",
    "for curr_thresh_idx, curr_thresh in enumerate(thresholds):\n",
    "    \n",
    "    canny_params = {\"fsize\":filter_size, \"thresholds\":curr_thresh}\n",
    "    img_canny = find_edges_canny(img, canny_params)\n",
    "    display_image(img_canny, axes=ax[curr_thresh_idx], cmap='gray')\n",
    "    ax[curr_thresh_idx].set_title(\" Range: {}\".format(curr_thresh), fontsize=30)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7510dab",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f1d0ea",
   "metadata": {},
   "source": [
    "In the previous cell, you found the edges of the noiseless image. \n",
    "\n",
    "\n",
    "**QUESTION:** (/4)\n",
    "\n",
    "In the next cell, find the thresholds that give the best edges for the images `matterhorn_noise1.jpg` and `matterhorn_noise2.jpg`. Justify your choice, and compare your results with the homemade edge detector you implemented previously. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf191f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load images\n",
    "imgs = [cv.imread(os.path.join('..','data', 'matterhorn_noise1.jpg'), cv.IMREAD_GRAYSCALE),\n",
    "        cv.imread(os.path.join('..','data', 'matterhorn_noise2.jpg'), cv.IMREAD_GRAYSCALE)]\n",
    "\n",
    "imgs_name = [\"S&P Noise\", \"Gaussian Noise\"]\n",
    "\n",
    "# Best detector parameters \n",
    "\n",
    "\n",
    "###################\n",
    "# YOUR CODE HERE\n",
    "canny_params_sp = {}\n",
    "canny_params_g = {}    \n",
    "###################\n",
    "\n",
    "# Edge detection\n",
    "canny_params = [canny_params_sp, canny_params_g]\n",
    "\n",
    "fig, ax = plt.subplots(len(imgs), 2, figsize=(20, 10*len(imgs)))\n",
    "\n",
    "for curr_img_idx, curr_img in enumerate(imgs):\n",
    "    img_edges_canny = find_edges_canny(img, canny_params[curr_img_idx])\n",
    "\n",
    "    display_image(curr_img, axes=ax[curr_img_idx, 0], cmap='gray')\n",
    "    ax[curr_img_idx, 0].set_title(imgs_name[curr_img_idx], fontsize=30)\n",
    "    display_image(img_edges_canny, axes=ax[curr_img_idx, 1], cmap='gray')\n",
    "    ax[curr_img_idx, 1].set_title(\"{} - Edges\".format(imgs_name[curr_img_idx]), fontsize=30)\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a60be",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b91daaf",
   "metadata": {},
   "source": [
    "## 1.3 Introduction to Histogram of Gradient (*HoG*)\n",
    "\n",
    "(30 points)\n",
    "\n",
    "### 1.3.1 Introduction\n",
    "\n",
    "Histogram of Oriented Gradients (*HOGs*) are descriptors mainly used in computer vision and machine learning for object detection. HOG features were first introduced by Dalal and Triggs in their CVPR 2005 paper: \"Histogram of Oriented Gradients for Human Detection\" (<http://ieeexplore.ieee.org/document/1467360/>); which transforms image pixels into a vector representation that is sensitive to broadly informative image features regardless of confounding factors like illumination. Later on, we will use these descriptors for detection and tracking. \n",
    "\n",
    "\n",
    "In their work, Dalal and Triggs proposed HOG and a 5-stages descriptor to classify humans in static images.\n",
    "The 5 stages included:\n",
    "\n",
    "    1. Pre-processing and scaling.\n",
    "    2. Computing gradients in both the x and y directions.\n",
    "    3. Obtaining weighted votes in spatial cells (local histograms).\n",
    "    4. Contrast normalizing overlapping spatial cells (Blocks).\n",
    "    5. Collecting all Histograms of Oriented gradients to form the final feature vector.\n",
    "\n",
    "\n",
    "In most real-world applications, HOG is used in conjunction with a [__Linear SVM__](https://en.wikipedia.org/wiki/Support_vector_machine) to perform object detection. HOG rapidly became one of the most used descriptors in image classification. The reason HOG is employed so heavily is because the local object appearance and shape can be characterized using the distribution of local intensity gradients. \n",
    "\n",
    "We’ll be discussing the steps necessary to combine both HOG and a Linear SVM into an object classifier later in this course. But for now, just understand that HOG is mainly used as a descriptor for object detection and that later these descriptors can be fed into a machine learning classifier.\n",
    "\n",
    "\n",
    "### 1.3.2 Objectives of this module\n",
    "\n",
    "HOG based classifiers are already implemented in several methods inside OpenCV. In this module, we will follow the basic steps to construct a feature vector based on HOG from scratch. Once the basic concepts are clear, at the end of this notebook, we will include the OpenCV build-in function used in real-world applications. The parameters of this function should be then clear for you to tune and play with for later applications.\n",
    "\n",
    "*What are HOG descriptors used to describe?*\n",
    "\n",
    "HOG descriptors are mainly used to describe the structural shape and appearance of an object in an image, making them excellent descriptors for object classification. In addition, since HOG captures local intensity gradients and edge directions, it also makes them good as texture descriptors.\n",
    "\n",
    "\n",
    "### 1.3.3 Preprocessing\n",
    "\n",
    "Typically, a feature descriptor converts an image of size $ width \\times height \\times 3 $(channels) to a feature vector array of length $n$. In the case of our implementation of the HOG feature descriptor, the input will be a grayscale image of size 64 x 128 and the output feature vector of a length that will depend on various choices that we will make along the notebook. \n",
    "\n",
    "Before any computation, we must ensure the input image has the right shape. \n",
    "\n",
    "**QUESTION:** (/2)\n",
    "\n",
    "In the next cell, convert the image `lena.png` into the desired format, i.e. a grayscale image of size (64x128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d393b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loads base image\n",
    "img = cv.imread(os.path.join('..','data', 'lena.png'))\n",
    "print('Input image has dimensions: {}'.format(img.shape))\n",
    "\n",
    "# converts to grayscale and resize\n",
    "\n",
    "###################\n",
    "# YOUR CODE HERE \n",
    "img_gray= \n",
    "###################\n",
    "\n",
    "print('Final image has dimensions: {}'.format(img_gray.shape))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "display_image(img, axes=ax[0])\n",
    "ax[0].set_title(\"Original image\", fontsize=30)\n",
    "display_image(img_gray, axes=ax[1], cmap='gray')\n",
    "ax[1].set_title(\"Image for HoG\", fontsize=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6a529",
   "metadata": {},
   "source": [
    "### 1.3.4 Gradient computation\n",
    "\n",
    "https://en.wikipedia.org/wiki/Image_gradient\n",
    "\n",
    "Gradient vectors (or “image gradients”) are one of the most fundamental concepts in computer vision; many vision algorithms involve computing gradient vectors for each pixel in an image. A gradient vector it’s simply a measure of the change in pixel values along the x-direction and the y-direction around each pixel. The magnitude is defined as:\n",
    "\n",
    "$$\n",
    "G(x,y) = \\sqrt{(\\Delta x^2 + \\Delta y^2)}\n",
    "$$\n",
    "\n",
    "and the phase: \n",
    "\n",
    "$$\n",
    "\\theta(x,y) = atan(\\frac{\\Delta x}{\\Delta y})\n",
    "$$\n",
    "\n",
    "Where: $\\Delta x = f(x+1,y) - f(x-1,y)$ and $\\Delta y = f(x,y+1) - f(x,y-1)$, are simply the directional change from one pixel to the other.\n",
    "\n",
    "The simplest implementation of the aforementioned operators computes each value by using a mask operator: $-1|0|1$ over the pixel position in each direction. However the Sobel operator you implemented in Section 1.2.2is usually employed as it is more robust to intensity changes.\n",
    "\n",
    "**QUESTION** (/2)\n",
    "\n",
    "In the next cell, use the function `apply_sobel_filter` on the image `img_gray` to compute the gradient of the image in the `x` and `y` directions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7261de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filter_size = 1\n",
    "\n",
    "# Sobel filtering\n",
    "\n",
    "\n",
    "###################\n",
    "# YOUR CODE HERE\n",
    "gx =\n",
    "gy = \n",
    "###################\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "obj = display_image(gx, axes=ax[0])\n",
    "ax[0].set_title('Derivative along X direction', fontsize=30)\n",
    "fig.colorbar(obj, orientation ='vertical', ax=ax[0])\n",
    "obj = display_image(gy, axes=ax[1])\n",
    "ax[1].set_title('Derivative along Y direction', fontsize=30)\n",
    "fig.colorbar(obj, orientation ='vertical', ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813eb1fe",
   "metadata": {},
   "source": [
    "**QUESTION**: (/2)\n",
    "- Do the images above make sense?  Explain what each image displayed. \n",
    "- What do the brightest or darkest pixels mean for each direction?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3de7d",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fee019",
   "metadata": {},
   "source": [
    "As mentionned in the introduction, the images above shows the gradient in a given direction. We can use them to compute the magnitude and phase as in the above formulas.\n",
    "\n",
    "**QUESTION**: (/1)\n",
    "\n",
    "In the next cell, compute the magnitude and the phase (in degrees) of each pixel from the gradient maps `gx` and `gy` computed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d7036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###################\n",
    "# YOUR CODE HERE\n",
    "magnitude = \n",
    "phase = \n",
    "###################\n",
    "\n",
    "# Plots of the magnitude and the phase:\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 5))\n",
    "\n",
    "#The magnitude is shown in gray scale.\n",
    "obj = display_image(magnitude, axes=ax[0])\n",
    "ax[0].set_title('Gradient Magnitude')\n",
    "fig.colorbar(obj, orientation ='vertical', ax=ax[0])\n",
    "\n",
    "# The phase is shown using a color map. \n",
    "obj = display_image(phase, axes=ax[1], cmap='hsv')\n",
    "ax[1].set_title('Gradient Phase (in degrees)')\n",
    "fig.colorbar(obj, orientation ='vertical', ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131dd2db",
   "metadata": {},
   "source": [
    "From the image above, we see that the phase is computed from 0 to 360 degrees. While this is conceptually correct, two pixels with very similar angular values towards the X-axis (let's say 0.1 radians and 6.2 radians) will be considered very different by a naive classifier. We correct this behavior by wrapping the phase from 0 to $\\pi$ (0 to 180 degrees). At the end of the notebook, you will correct the phase range.\n",
    "\n",
    "\n",
    "**QUESTION:** (/1)\n",
    "\n",
    "From the phase image, we see many red pixels (values very close to zero). Interpret this observation.\n",
    "\n",
    "**Hint:** consider the values of gx and gy in those regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f932bb",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a2683",
   "metadata": {},
   "source": [
    "### 1.3.5 Computation of the Histograms of the Phase for Local Cells\n",
    "\n",
    "In the next step, you will compute the histograms of Oriented Gradients, which are histograms of the phase distribution in a region of the image. We split the image into cells, rectangular regions of interest sliding across the image domain. The size and shape of the cell will affect the performance of HOG as a feature vector. For each cell, you will compute a histogram. In this work, you will use $8 px\\times 8px$ cells. \n",
    "\n",
    "In the next cell, we show you the cell subdivisions of the (color) image for cells of size $8\\times 8$ and the histogram of the first cell of the image (starting at the pixel (0, 0)) with 36 bins. The number of bins considered will affect the performance and quality of the descriptor. In the original paper, Dalal and Triggs found that nine bins (from 0 to 180 degrees) performed well for human detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a13f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This parameter is usually constant and it's never changed true the procedure.\n",
    "CELL_SIZE = 8;\n",
    "\n",
    "# Histogram of the Phase for the very first cell\n",
    "num_of_bins = 9\n",
    "\n",
    "# Compute histogram\n",
    "hist = cv.calcHist([phase[:CELL_SIZE, :CELL_SIZE].astype(np.float32)], channels=[0], mask=None, histSize=[num_of_bins], ranges=[0,360])\n",
    "\n",
    "# Example of a cell drawn in yellow\n",
    "img_canvas = cv2.resize(img, (64, 128)) # temporal image.\n",
    "\n",
    "for stride_x in range(1,img.shape[0]-1):\n",
    "    for stride_y in range(1,img.shape[1]-1):\n",
    "        cv2.rectangle(img_canvas, (0 + (stride_x-1)*CELL_SIZE, 0 + (stride_y-1)*CELL_SIZE), (0 + stride_x*CELL_SIZE, 0 + stride_y*CELL_SIZE), (0, 255, 255))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "display_image(img_canvas, axes=ax[0])\n",
    "\n",
    "ax[1].bar(np.arange(hist.shape[0]), hist.reshape(-1), width=1)\n",
    "ax[1].set_xlabel('Orientation')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_title('Orientation distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845b366",
   "metadata": {},
   "source": [
    "The previous cell shows you how to compute the histogram of one cell, but we need the histogram of each cell.\n",
    "\n",
    "**QUESTION:** (/5)\n",
    "\n",
    "In the next cell, implement the function `computeCellHistograms()` that takes as input the phase of the image, the number of bins, and the cell size, and returns a [list](https://docs.python.org/3/tutorial/datastructures.html) containing the histograms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCellHistograms(phase, cell_size, num_of_bins):\n",
    "    \"\"\"\n",
    "    Compute histogram for each cells and concatenate them.\n",
    "    \n",
    "    :param phase: 1 channel image with the gradient direction.\n",
    "    :params cell_size: cell size (8px) \n",
    "    :params num_of_bins: number of bins in the histogram\n",
    "    :return concatenated_histograms: list of histograms\n",
    "    \"\"\"\n",
    "    \n",
    "    # Resulting histogram list\n",
    "    concatenated_histograms =[];\n",
    "    \n",
    "    # Compute the histogram for each cell\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "     \n",
    "    return concatenated_histograms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac60e80",
   "metadata": {},
   "source": [
    "The next cell tests your implementation.\n",
    "\n",
    "**QUESTION**: (/1)\n",
    "\n",
    "What should be the number of histograms? Is it correct?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f0be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute histogram\n",
    "histograms = computeCellHistograms(phase, CELL_SIZE, num_of_bins)\n",
    "\n",
    "print(\"Number of histograms: {}\".format(len(histograms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf3ac9c",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a308f8",
   "metadata": {},
   "source": [
    "The function `computeCellHistograms()` computes the histogram of each cell. However, it assumes that each direction has the same importance and contributes equally to the histogram. In the original paper, they proposed to weight the histogram, for instance, using the gradient magnitude. The histogram bins of each cell are weighted by the gradient magnitude. The effect is depicted below:\n",
    "\n",
    "<img src=\"https://gurus.pyimagesearch.com/wp-content/uploads/2015/03/hog_histogram_animation.gif\">\n",
    "<em> Image taken from: https://gurus.pyimagesearch.com </em>\n",
    "\n",
    "The effect of normalizing the weights will directly affect the contribution of each direction.\n",
    "\n",
    "**QUESTION:** (/5)\n",
    "\n",
    "In the next cell, implement the function `computeCellWeightedHistogram()` to return weighted histograms. The new version takes as input the phase of the image, the cell size, the number of bins, and the gradient magnitude and returns a [list](https://docs.python.org/3/tutorial/datastructures.html) containing the histograms. \n",
    "\n",
    "**HINT:** Avoid pre-defined functions to compute the histogram and code your own function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb8838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCellWeightedHistogram(phase, magnitude, cell_size, num_of_bins):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute weighted histogram for each cells and concatenate them.\n",
    "    \n",
    "    :param phase: 1 channel image with the gradient direction.\n",
    "    :param magnitude: 1 channel image with the gradient magnitude.\n",
    "    :params cell_size: cell size (8px) \n",
    "    :params num_of_bins: number of bins in the histogram \n",
    "    :return weigthed_and_concatenated_histograms: list of weighted histograms\n",
    "    \"\"\"\n",
    "\n",
    "    weigthed_and_concatenated_histograms = []\n",
    "    \n",
    "    \n",
    "    # CODE HERE\n",
    "            \n",
    "    return weigthed_and_concatenated_histograms;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cbf492",
   "metadata": {},
   "source": [
    "The next cell shows you the histogram and the weighted histogram of the first cell. \n",
    "\n",
    "**QUESTION:** (/3)\n",
    "\n",
    "Compare and comment the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b55dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weighted histogram\n",
    "weightedHistograms = computeCellWeightedHistogram(phase, magnitude, CELL_SIZE, num_of_bins)\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "ax[0].bar(np.arange(histograms[0].shape[0]), histograms[0].reshape(-1), width=1)\n",
    "ax[0].set_xlabel('Orientation')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_title('Orientation distribution')\n",
    "\n",
    "\n",
    "ax[1].bar(np.arange(weightedHistograms[0].shape[0]), weightedHistograms[0].reshape(-1), width=1)\n",
    "ax[1].set_xlabel('Orientation')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_title('Orientation distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cba6b",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae84ba",
   "metadata": {},
   "source": [
    "### 1.3.6 Block normalization\n",
    "\n",
    "In the previous steps, we created the oriented histogram based on the gradient magnitude. However, gradients are sensitive to overall lighting. If you make the image darker by dividing all pixel values by 2, for example, the gradient magnitude will change by half, and, therefore, the histogram values will change by half. Ideally, we want our descriptor to be independent of lighting variations. In other words, we would like to “normalize” the histograms to stabilitze the results. \n",
    "\n",
    "Rather than normalizing each histogram individually, we group the cells into __blocks__ and normalize the histogram of the block instead of the indivual cell. These procedure makes the feature detector less sensitive to intensity changes.\n",
    "\n",
    "\n",
    "<img src = \"https://gurus.pyimagesearch.com/wp-content/uploads/2015/03/hog_contrast_normalization.gif\">\n",
    "<em> Image taken from: https://gurus.pyimagesearch.com </em>\n",
    "\n",
    "Image above shows each block as a group of 4 cells (2 by 2 cells). The blocks overlap each other and, therefore, each cell is represented in the histograms multiple times. This redundancy actually improves accuracy. Finally, we take the resulting block histograms, concatenate them, and treat them as our final feature vector.\n",
    "\n",
    "\n",
    "**QUESTION:** (/6)\n",
    "\n",
    "In the next cell, implement the function `BlockNormalizationAndFeatureConcatenation()` that takes as input the histograms, and the image size and returns the feature vector of histogram normalized with the L2 norm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf849db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BlockNormalizationAndFeatureConcatenation(histograms, img_size, cell_size, NB_BLOCK_CELLS_X = 2, NB_BLOCK_CELLS_Y = 2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute histogram  block normalization\n",
    "    \n",
    "    :param histograms: list of weighted histograms\n",
    "    :params img_size:  size of the image\n",
    "    :return \n",
    "    \"\"\"\n",
    "    \n",
    "    feature_vector    = []\n",
    "    \n",
    "    #CODE HERE\n",
    "            \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc5866",
   "metadata": {},
   "source": [
    "The next cell shows you the histogram of the first cell, the weighted histogram of the first cell and the histogram of the first block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature vector\n",
    "feature_vector = BlockNormalizationAndFeatureConcatenation(weightedHistograms, img_gray.shape, CELL_SIZE)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "ax[0].bar(np.arange(histograms[0].shape[0]), histograms[0].reshape(-1), width=1)\n",
    "ax[0].set_xlabel('Orientation')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_title('Orientation distribution')\n",
    "\n",
    "ax[1].bar(np.arange(weightedHistograms[0].shape[0]), weightedHistograms[0].reshape(-1), width=1)\n",
    "ax[1].set_xlabel('Orientation')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_title('Orientation distribution')\n",
    "\n",
    "ax[2].bar(np.arange(feature_vector[0].shape[0]), feature_vector[0].reshape(-1), width=1)\n",
    "ax[2].set_xlabel('Orientation')\n",
    "ax[2].set_ylabel('Count')\n",
    "ax[2].set_title('Orientation distribution')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa33b2",
   "metadata": {},
   "source": [
    "### 1.3.7 Feature Vector Visualization\n",
    "\n",
    "The final descriptor is then a 1-dimentional array with the concatenation of the blocks normalized histograms. However, this shouldn't prevent us from having an intuitive visualization of the computed features. \n",
    "\n",
    "**QUESTION:**(/5)\n",
    "\n",
    "As last exercise, you need to display the feature vector over the image space (or any clever way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13091e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1def1cc",
   "metadata": {},
   "source": [
    "### 1.3.8 HOG in action\n",
    "\n",
    "As you may suspect already, OpenCV comes with a nice implementation of the HOG descriptor. Once you finish this notebook, you should be able to understand the basic parameters from the `cv.HOGDescriptor()` class.\n",
    "\n",
    "In the next cell, we use a simple Support Vector Machine algoritmh (SVM) to detect People in multi-scale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5753db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the HOG descriptor/person detector\n",
    "hog = cv.HOGDescriptor()\n",
    "hog.setSVMDetector(cv.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# load base image (check that we are not scaling, normalizing or changing the channels)\n",
    "img = cv.imread(os.path.join('..','data', 'person_104.bmp'))\n",
    "print('Image has dimensions: {}'.format(img.shape))\n",
    "\n",
    "# The HOG detector returns an array with the Regions of maximum likehood to contain a human-shaped-form\n",
    "rects, weights = hog.detectMultiScale(img, winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "\n",
    "# draw the original bounding boxes\n",
    "persons = 0;\n",
    "for (x, y, w, h) in rects:\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, persons * 256), 2)\n",
    "    persons += 1;\n",
    "    \n",
    "display_image(img);\n",
    "plt.title('Detection results')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base image\n",
    "img = cv.imread(os.path.join('..','data', 'person_454.bmp'))\n",
    "print('Image has dimensions: {}'.format(img.shape))\n",
    "\n",
    "# The HOG detector returns an array with the Regions of maximum likehood to contain a human-shaped-form\n",
    "rects, weights = hog.detectMultiScale(img , winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "\n",
    "# draw the original bounding boxes\n",
    "persons = 0;\n",
    "for (x, y, w, h) in rects:\n",
    "    cv2.rectangle(img , (x, y), (x + w, y + h), (0, 255, persons * 256), 2)\n",
    "    persons += 1;\n",
    "    \n",
    "display_image(img);\n",
    "plt.title('Detection results')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab2862f",
   "metadata": {},
   "source": [
    "### 1.3.9 References\n",
    "\n",
    "https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\n",
    "\n",
    "https://www.learnopencv.com/histogram-of-oriented-gradients/\n",
    "\n",
    "http://juliaimages.github.io/ImageFeatures.jl/latest/tutorials/object_detection.html\n",
    "\n",
    "https://www.learnopencv.com/tag/hog/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
