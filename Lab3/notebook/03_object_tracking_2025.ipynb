{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RxV0LVhSS1a"
   },
   "source": [
    "This aim of this course is to review the evolution of image processing tools from hand-crafted methods to deep learning algorithms. The semester is split into four labs :\n",
    "\n",
    "* **Lab 1** : Introduction to Image Processing Using Hand-Crafted Features\n",
    "* **Lab 2** : Object detection\n",
    "* **Lab 3** : Object tracking\n",
    "* **Lab 4** : Introduction to Deep Learning for image classification and generative model\n",
    "\n",
    "Let's start with the third chapter of this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kva6jQnfSC84"
   },
   "source": [
    "# Chapter 3 : Object Tracking \n",
    "(100 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "executionInfo": {
     "elapsed": 606,
     "status": "error",
     "timestamp": 1619414748325,
     "user": {
      "displayName": "Rémy Gardier",
      "photoUrl": "",
      "userId": "05844193576466295019"
     },
     "user_tz": -120
    },
    "id": "-bwD4IwvSC9A",
    "outputId": "d6f17fed-d894-4830-90d5-1741da653d2d"
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import tarfile\n",
    "import time as _time\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "c1jx0nJ7SC9B"
   },
   "source": [
    "## 1 Video Processing and Detection. \n",
    "\n",
    "In this first part of chapter 3, you will be asked to construct a series of functions tools to handle video processing and basic detection. You will use the same HOG+SVM classifier from Chapter 2 and adapt it to handle video input.\n",
    "\n",
    "In contrast to the previous assignments, we will not provide a guided skeleton of the function. You will use any high level (already implemented) function from OpenCV to complete the exercise specifications and outputs. In addition, you will be asked to search in the documentation (or your favourite reference) the specific usage of the functions. \n",
    "\n",
    "__Section Objectives:__\n",
    "\n",
    "* Construct a Video processing pipeline.\n",
    "* Use OpenCV functions on the video frames for human detection.\n",
    "* Analyse and compare tracking methodes proposed by OpenCV\n",
    "\n",
    "__Data__:\n",
    "\n",
    "The folder ``../data/videos`` contains three sample videos to test your functions.\n",
    "\n",
    "\n",
    "### 1.1 Reading/Writing video (5 pts)\n",
    "\n",
    "A video file can be abstracted as a bunch of images of the same dimensions in order (*i.e. collection*). \n",
    "\n",
    "**QUESTION**\n",
    "\n",
    "Your first task is to complete the function `transform_video_file(...)` which will operate on every frames of a given video file. The following steps are required:\n",
    "\n",
    "- Open video file with OpenCV\n",
    "- Iterate overall frames or a subset of them (based on `n_frame` parameter)\n",
    "- Apply the processing function. The function's signature needs to be `function(np.array, Any) -> Any`\n",
    "\n",
    "To read videos you can use the OpenCV build-in functions, namely `cv::VideoCapture` class. More informations is available in the [docs](https://docs.opencv.org/3.4.4/d8/dfe/classcv_1_1VideoCapture.html#a473055e77dd7faa4d26d686226b292c1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Jhd4SxeSC9C"
   },
   "outputs": [],
   "source": [
    "def transform_video_file(file_path, function, params=None, n_frame=-1):\n",
    "    \"\"\"\n",
    "    Given the path of a video file (file_path) the function reads every frame of the input video and applies a given\n",
    "    transformation (function) using the parameters (params)\n",
    "\n",
    "    :input_image:       Input video file path \n",
    "    :function:          Function be applied to each frame of the image. Signature `function(np.array, Any) -> Any`\n",
    "    :params:            Any parameter needed for the function above.\n",
    "    :n_frame:           Maxiumum number of frame to read. Default `-1`, read all the content\n",
    "    :return:            output_handler this can be anything you may need to save your results.\n",
    "    \"\"\"\n",
    "    output_handler = []\n",
    "    # Open video\n",
    "    cap = cv.VideoCapture(file_path)\n",
    "    if cap.isOpened():\n",
    "        \n",
    "        #######################\n",
    "        # YOUR CODE HERE\n",
    "        #######################\n",
    "\n",
    "        pass\n",
    "                \n",
    "    else:\n",
    "        raise ValueError('Can not open file: {}'.format(file_path))\n",
    "    # Close reader\n",
    "    cap.release()\n",
    "    # Return custom structure\n",
    "    return output_handler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ8tsEhRSC9D"
   },
   "source": [
    "### 1.2 Example\n",
    "\n",
    "The example below shows how the `transform_video_file` can be used. It will open the video named `speaker.avi` and process only the first **2** frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWIHmh7JSC9E",
    "outputId": "69f19588-b592-4d74-b145-f841b8ef5113"
   },
   "outputs": [],
   "source": [
    "# Example of usage:\n",
    "def dummy_function(image, params):\n",
    "    if('dummy_function' in params[0]):\n",
    "        print('Func: \"{}\", Message \"{}\", File: \"{}\"'.format(params[0], params[1], params[2]))\n",
    "        print('Image dims: {}'.format(image.shape))\n",
    "    return 'I did it'\n",
    "    \n",
    "# Define parameters\n",
    "file_name = '../data/videos/speaker.avi'\n",
    "extra_params = ['dummy_function', 'Hello from the video file', file_name];\n",
    "\n",
    "# Process single frame\n",
    "transform_video_file(file_path=file_name, function=dummy_function, params=extra_params, n_frame=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Testing your function (5 pts)\n",
    "\n",
    "To test your function you need to process the video: ``../data/videos/video_gym.avi``. \n",
    "\n",
    "**QUESTION**\n",
    "\n",
    "At each frame, you will count the number of __green__ pixels. At the end you should return two outputs:\n",
    "\n",
    "* The __frame number__ with the maximum number of green pixels\n",
    "* The __image__ frame with more green pixels. You are asked to display the original frame with the green pixels **replaced** by their grayscale value.\n",
    "\n",
    "Display the image and the frame number. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o63nZHgMSC9H",
    "outputId": "b19228ed-12fd-4536-ea33-ad372b36521b"
   },
   "outputs": [],
   "source": [
    "# Frame processing function\n",
    "def processing_fn(image, params):\n",
    "    \"\"\"\n",
    "    Count the number of colored pixels in a given image. This function also extracts the region where the pixels are green\n",
    "    in form of a mask.\n",
    "    \n",
    "    :param image:  Frame to analyse\n",
    "    :param params: Extra parameters that my be required by the functions\n",
    "    \"\"\"\n",
    "    nb_green_pix = 0 \n",
    "    mask    = None\n",
    "    #######################\n",
    "    # YOUR CODE HERE\n",
    "    ######################\n",
    "    \n",
    "    # Done\n",
    "    return (nb_green_pix, mask) # Magic structure with the number of green pixels + mask\n",
    "\n",
    "\n",
    "# Video file\n",
    "file_path   =  os.path.join('..','data', 'videos', 'video_gym.avi')\n",
    "\n",
    "# Find and display frame with max number of green pixels\n",
    "    \n",
    "#################################\n",
    "# YOUR CODE HERE\n",
    "#################################\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GMpclWVYSC9H"
   },
   "source": [
    "\n",
    "**QUESTION**\n",
    "\n",
    "Secondly, at each 30 seconds of the video, show the following results\n",
    "\n",
    "- Extracted mask\n",
    "- Number of green pixels counted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqMjxSa_SC9I",
    "outputId": "bad7a94a-fc39-4df7-eaa6-faea50ca8dd1"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#################################\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OTfs0doFSC9J"
   },
   "source": [
    "### 1.4 The return of the HOG. (5 pts)\n",
    "\n",
    "Do you remember HOG? Here is a small reminder how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3di9Y1USC9K",
    "outputId": "9c72955b-5dcd-4b37-e7ed-30d01a500b01"
   },
   "outputs": [],
   "source": [
    "# initialize the HOG descriptor/person detector\n",
    "hog = cv.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# load base image (check that we are not scaling, normalizing or changing the channels)\n",
    "img = cv.imread(os.path.join('..','data', 'person_454.bmp'))\n",
    "print('Image has dimensions: {}'.format(img.shape))\n",
    "\n",
    "# The HOG detector returns an array with the Regions of maximum likehood to contain a human-shaped-form\n",
    "rects, weights = hog.detectMultiScale(img , winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "\n",
    "# draw the original bounding boxes\n",
    "persons = 0;\n",
    "for k, (x, y, w, h) in enumerate(rects):\n",
    "    cv2.rectangle(img , (x, y), (x + w - 1, y + h -1), (0, 255, k * 256), 2)\n",
    "    \n",
    "display_image(img);\n",
    "plt.title('Detection results')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "F2vL-chhSC9L"
   },
   "source": [
    "Finally, you will glue together your brand new image processing function and the HOG descriptor from OpenCV above. You are asked to write a function `detection_fn` to perform person detection, and test your implementation.\n",
    "\n",
    "### Testing your function\n",
    "\n",
    "To test your function you need to process the video: ``../data/videos/video_skater.avi``.\n",
    "\n",
    "**QUESTION**\n",
    "\n",
    " At each frame, you will use HOG to detect any person in the frame. Your function should return: \n",
    "\n",
    "* The image __frame__ corresponding to __the seconds 1, 2, ... , 10__ of the video with the rectangle showing the \"detected area\" if any. \n",
    "\n",
    "\n",
    "__Be sure to display all the 10 frames for grading__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCmuNu68SC9L"
   },
   "outputs": [],
   "source": [
    "# 1. Write detection_fn\n",
    "\n",
    "def detection_fn(detector, image, params):\n",
    "    \"\"\"\n",
    "    Run pedestrian detector on a given image.\n",
    "    \n",
    "    :param detector: HOG Detector instance\n",
    "    :param image: Image to process\n",
    "    :param params: Extra parameters needed by the function\n",
    "    :return: Detected pedestrian bounding boxes\n",
    "    \"\"\"\n",
    "    \n",
    "    box     = None\n",
    "    frame   = None\n",
    "    \n",
    "    ########################\n",
    "    # YOUR CODE HERE\n",
    "    ########################\n",
    "    return frame, box\n",
    "    \n",
    "#2. Test the implementation\n",
    "# Video file\n",
    "file_path   =  os.path.join('..','data', 'videos', 'video_skater.avi')\n",
    "outputs     = transform_video_file(file_path, lambda im, p: detection_fn(hog, im, p), n_frame=350);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-U8ObCwDSC9L",
    "outputId": "4845ce95-4d85-4f3b-8632-07e12d0ec968",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Display your images here\n",
    "########################\n",
    "# YOUR CODE HERE\n",
    "########################\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QO-ZEh20SC9M"
   },
   "source": [
    "## 2 Tracking - Particles Filter\n",
    "\n",
    "A **particles filter** is an object tracking algorithm which try to ***estimate*** the real position of an object approximating its position in the search space (i.e. image space) using a set of particles. A particle has two properties:\n",
    " \n",
    "    - A state: An estimation of the quantity being tracked, in our case it will be a tuple(x, y) that represent the possible location of the object in the image space.\n",
    "    - A weight: A probability that the particle is at the correct location of the object being tracked\n",
    "    \n",
    "The goal of the filter is to estimate the probability density function of the quantity being tracked over the whole search space. The set or particles is an approximation of this probability density function. When there are a lot of particles in a given region, the probability of the object being there is high. \n",
    "To estimate this distribution the algorithm uses Monte Carlo techniques and **approximate** this quantity. There is a total of three steps required to implement a particles filter:\n",
    " \n",
    "1. Perturbate: Given a set of particles, the state is slightly perturbed in order to explore the neighbourhood of each particle. This perturbation is generally drawn from a gaussian distribution.\n",
    "2. Reweight: The particle's weight are updated in this step. Given a set of measurements (*i.e. perturbed particles*) a similarity metric is computed between the measurements and the object model (*i.e. template*). Particle with high similarity will have a large weight (*i.e. the measurement looks very much like the object*) whether a particle with low similarity will be assigned a small weight (*i.e. the measurement does not look like the object*). The reweighting step adapts the estimation of the probability density function of the quantity being tracked.\n",
    "3. Resample: The set of particles is updated by sampling in the previous collection of particles using the newly estimated probability density function. Since particles similar to the object have a higher chance to be chosen, the new set of particles should have states located on the actual object.\n",
    " \n",
    "In the following section you will implement these three steps from scratch with the help of the class `ParticleFilterInterface` from file `utils.py`. This class has the following members that will be used in the different methods.\n",
    " \n",
    "- `self.model`: The template of the object being tracked, np.ndarray\n",
    "- `self.search_space`: The dimensions of the search space, tuple\n",
    "- `self.n_particles`: The total number of particles used to estimate the distribution, int\n",
    "- `self.state_dims`: The number of component of each state, int (*i.e. 2 in our case, x/y*), tuple\n",
    "- `self.sigma_perturbation`: Standard deviation of the perturbation applied to the particles, float\n",
    "- `self.sigma_similarity`: Standard deviation of the residual error used when computing similarity, float\n",
    "- `self.alpha`: Blending coefficient used during model adaptation phase, float\n",
    "- `self.particles`: Collection of particles states stored as 2D array, first column is x, second is y, np.ndarray\n",
    "- `self.weights`: Distribution estimation, 1D array\n",
    "- `self.state`: Current best estimation of the object position, np.ndarray\n",
    "- `self.indexes`: List of index for each particles, nd.array\n",
    " \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pQR4zDNiSC9N"
   },
   "source": [
    "### 2.1 Step 1 : Perturbation (15 pts)\n",
    " \n",
    "In the following cell, two functions need to be implemented to make the tracker work. The methods are: `_initialize_particles` and `perturbate_impl`.\n",
    " \n",
    "The first one, is responsible to initialize each particle's state and weight respectively. Particle's states are stored into a 2D array of dimensions **(#Particles, #States)**, where the first column correspond to the **x** coordinate in the image space the second column is the **y** coordinate. At initialization, each component of the states are uniformly sampled from the search space dimensions (*i.e. image dimensions*).\n",
    " \n",
    "Particles' weights are stored into a 1D array of dimension **(#Particles,)**. Each weight represents the probability of the particle being at the real location of the tracked object. At initialization, particles are equiprobable with a probability of $\\frac{1}{\\#Particles}$. Moreover the weights need to represent a valid probability distribution function and, therefore, they have to sum to one.\n",
    " \n",
    "The second method is responsible to add perturbation on the particle state. Each perturbation is sampled from a gaussian distribution defined as $\\mathcal{N}\\left(0, \\sigma_{p}^2 \\right)$. This perturbation is responsible for exploring the neighbourhood of each particle by randomly moving around.\n",
    " \n",
    "**QUESTION**\n",
    " \n",
    "In the following cell, implement the methods `_initialize_particles` and `perturbate_impl`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gfH0N9VSC9N"
   },
   "outputs": [],
   "source": [
    "class ParticleFilterStepOne(ParticleFilterInterface):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 search_space,\n",
    "                 n_particles,\n",
    "                 state_dims,\n",
    "                 sigma_perturbation=10.0,\n",
    "                 sigma_similarity=20.0,\n",
    "                 alpha=0.0,\n",
    "                 update_rate=10):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param model:         Template image to be tracked (numpy.ndarray)\n",
    "        :param search_space:  Possible size of the search space (i.e. image size)\n",
    "        :param n_particles:   Number of particle used to perform tracking\n",
    "        :param state_dims:    State space dimensions (i.e. number of parameters\n",
    "                              being tracked)\n",
    "        :param sigma_perturbation: How much each particle will be perturbated, float\n",
    "        :param sigma_similarity:  Similarity residual distribution\n",
    "        :param alpha:         Blending factor for model adaptation, float\n",
    "        :param update_rate:   Frequency at which the model will be updated, int\n",
    "        \"\"\"\n",
    "        super(ParticleFilterStepOne, self).__init__(model,\n",
    "                                                    search_space,\n",
    "                                                    n_particles,\n",
    "                                                    state_dims,\n",
    "                                                    sigma_perturbation,\n",
    "                                                    sigma_similarity,\n",
    "                                                    alpha)\n",
    "        self.state = None\n",
    "        self.update_rate = update_rate\n",
    "        self._initialize_particles()\n",
    "        self._check_particle_dims(init=True)\n",
    "        self._check_weights()\n",
    "        \n",
    "    def _initialize_particles(self):\n",
    "        \"\"\"\n",
    "        Initialize each particle state by sampling uniformly the whole serch space. Plus the each weight\n",
    "        associated to a particles is initialized with a uniform probability:\n",
    "        \"\"\"\n",
    "        \n",
    "        ########################\n",
    "        # YOUR CODE HERE\n",
    "        ########################\n",
    "        \n",
    "        \n",
    "        \n",
    "        return\n",
    "        \n",
    "    def update(self, frame):\n",
    "        \"\"\"\n",
    "        Update particle filter for a new frame\n",
    "        :param frame: New frame in the tracking sequence\n",
    "        :return:  Best state (i.e. estimated position\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Perturbation\n",
    "        self.perturbate()\n",
    "        \n",
    "        # Step 2:Other steps goes here\n",
    "        # ...\n",
    "        \n",
    "        self.state = self.current_state()\n",
    "        return self.state\n",
    "    \n",
    "    def perturbate_impl(self):\n",
    "        \"\"\" Perturbate particles with random noise \"\"\"\n",
    "        \n",
    "        ########################\n",
    "        # YOUR CODE HERE\n",
    "        ########################\n",
    "        \n",
    "        \n",
    "        return "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mMGH8yZpSC9N"
   },
   "source": [
    "The next cell creates a tracker that runs the initialization step, then displays the current particle state at that time.\n",
    " \n",
    "On the left side is your state initialization and on the right side is one possible solution which should be close to the one you have. It is there as a reference.\n",
    "\n",
    "**QUESTION** \n",
    "\n",
    "Why is the solution not exactly the same as the reference?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KdlztBFsSC9O",
    "outputId": "c187e2b8-837b-4b47-cd0c-b040cae795da"
   },
   "outputs": [],
   "source": [
    "# Load test image\n",
    "path = os.path.join('..','data', 'particles_filter', 'frame000.jpg')\n",
    "frame = cv.imread(path)\n",
    "\n",
    "# Define model\n",
    "miny = 175\n",
    "maxy = miny + 129\n",
    "minx = 321\n",
    "maxx = minx + 104\n",
    "model = frame[miny:maxy, minx:maxx]\n",
    "# Create particle filter\n",
    "pf = ParticleFilterStepOne(model=model,\n",
    "                           search_space=frame.shape[:2],\n",
    "                           n_particles=200,\n",
    "                           state_dims=2)\n",
    "# Draw particle's state\n",
    "canvas = pf.draw_particles(np.copy(frame))\n",
    "canvas_true = cv.imread(os.path.join('..','data', 'particles_filter', 'intialized_particles.jpg'))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 7))\n",
    "display_image(canvas, axes=ax[0])\n",
    "ax[0].set_title('Your initialization')\n",
    "display_image(canvas_true, axes=ax[1])\n",
    "ax[1].set_title('Expected behaviour')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GxdqKjXdSC9O"
   },
   "source": [
    "In the next cell, the tracker function `update` is called, it applies the first step of the tracking process. Meaning the particles will have a new state (*i.e. position*), to highlight this, the new particles are drawn in **red**. If it is **NOT** the case, check your code!\n",
    "\n",
    " \n",
    "**QUESTION**\n",
    "\n",
    "**Comment** on what you observe, does the particle state change ? Is the perturbation somehow what is expected ? How could you check that the implementation of the perturbation is correct? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0g70WQ-RSC9O",
    "outputId": "3417375b-0816-498a-cecc-ff1e5884c571"
   },
   "outputs": [],
   "source": [
    "# Apply first step of tracking\n",
    "pf.update(frame)\n",
    "\n",
    "# Draw particles, should have new position\n",
    "canvas_perturbated = pf.draw_particles(np.copy(canvas), color=(0, 0, 255))\n",
    "display_image(canvas_perturbated);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jmY6DCP_SC9P"
   },
   "source": [
    "### 2.2 Step Two : Reweighting (15 pts)\n",
    "\n",
    "The reweighting function performs two steps:\n",
    "    \n",
    "1. Measure the similarity between the image patch centered at every particle state and the object model.\n",
    "2. Update the weights probability distribution according the measured similarity\n",
    "\n",
    "**QUESTION** \n",
    "\n",
    "In the next cell, complete the implementation of the similarity function. To measure the similarity between two samples `x` and `y`, first the Mean Squared Error is computed then it is assumed that it follows a normal distribution in the form of N(0, sigma):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    r &= \\frac{1}{N} \\sum_{i=0}^{N} \\left(x_i - y_i\\right)^{2}, \\quad N = 3\\cdot H \\cdot W \\\\\n",
    "    s &= \\exp(-\\frac{r}{2\\sigma^{2}})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blKv93HPSC9P"
   },
   "outputs": [],
   "source": [
    "def similarity_fn(x, y, sigma):\n",
    "    \"\"\"\n",
    "    Compute similarity between `x` and `y` assuming the residual difference\n",
    "    follow a gaussian distribution N(0, sigma).\n",
    "    Special case, when the shape of `x`/`y` are not the same/empty it must return a similarity of 0.0\n",
    "    :param x: First image\n",
    "    :param y: Second image to compare\n",
    "    :param sigma: Standard deviation of the residual differences\n",
    "    :return: Scalar value, similarity between `x` and `y`\n",
    "    \"\"\"\n",
    "    sim = 0.0\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    # YOUR CODE HERE\n",
    "    ########################\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijk2G7BhSC9P",
    "outputId": "72a842ce-d732-432a-9d4a-370eb90b0958"
   },
   "outputs": [],
   "source": [
    "# Test your similarity_fn\n",
    "patch1 = np.copy(model)\n",
    "patch2 = frame[miny+5:maxy+5, minx+5:maxx+5]\n",
    "patch3 = np.asarray([])\n",
    "\n",
    "msg = 'Something is wrong with your similarity function, check you math'\n",
    "sim_1 = similarity_fn(patch1, patch1, sigma=40.0)\n",
    "print('Similarity between the same patch: {:.2f}'.format(sim_1))\n",
    "assert np.round(sim_1, 2) == 1.00, msg\n",
    "sim_2 = similarity_fn(patch1, patch2, sigma=40.0)\n",
    "print('Similarity between two different patches: {:.2f}'.format(sim_2))\n",
    "assert np.round(sim_2, 2) == 0.70, msg\n",
    "sim_3 = similarity_fn(patch1, patch3, sigma=40.0)\n",
    "print('Similarity between two different size patches: {:.2f}'.format(sim_3))\n",
    "assert np.round(sim_3, 2) == 0.00, msg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_wGyR1iYSC9Q"
   },
   "source": [
    "**QUESTION** \n",
    "\n",
    "In the next cell, implement the `reweight_impl` function responsible to update the probability distribution of the particles. The following steps are required\n",
    "\n",
    "- Extract patches of the frame centered at the particle's state\n",
    "- Compute similarity between the `model` and every patches\n",
    "- Normalize the `weights` to have a valid probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enH0kEw3SC9Q"
   },
   "outputs": [],
   "source": [
    "class ParticleFilterStepTwo(ParticleFilterStepOne):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 search_space,\n",
    "                 n_particles,\n",
    "                 state_dims,\n",
    "                 sigma_perturbation=10.0,\n",
    "                 sigma_similarity=20.0,\n",
    "                 alpha=0.0,\n",
    "                 update_rate=10):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param model:         Template image to be tracked (numpy.ndarray)\n",
    "        :param search_space:  Possible size of the search space (i.e. image size)\n",
    "        :param n_particles:   Number of particle used to perform tracking\n",
    "        :param state_dims:    State space dimensions (i.e. number of parameters\n",
    "                              being tracked)\n",
    "        :param sigma_perturbation: How much each particle will be perturbated, float\n",
    "        :param sigma_similarity:  Similarity residual distribution\n",
    "        :param alpha:         Blending factor for model adaptation, float\n",
    "        :param update_rate:   Frequency at which the model will be updated, int\n",
    "        \"\"\"\n",
    "        super(ParticleFilterStepTwo, self).__init__(model,\n",
    "                                                    search_space,\n",
    "                                                    n_particles,\n",
    "                                                    state_dims,\n",
    "                                                    sigma_perturbation,\n",
    "                                                    sigma_similarity,\n",
    "                                                    alpha)\n",
    "        \n",
    "    def update(self, frame):\n",
    "        \"\"\"\n",
    "        Update particle filter for a new frame\n",
    "        :param frame: New frame in the tracking sequence\n",
    "        :return:  Best state (i.e. estimated position\n",
    "        \"\"\"\n",
    "        self.perturbate()\n",
    "        self.reweight(frame)\n",
    "        # Other steps goes here\n",
    "        # ...\n",
    "        \n",
    "        self.state = self.current_state()\n",
    "        return self.state\n",
    "    \n",
    "    def reweight_impl(self, frame):\n",
    "        \"\"\"\n",
    "        Update particle's weight for the current frame\n",
    "        :param frame: New frame in the tracking sequence\n",
    "        \"\"\"\n",
    "        \n",
    "        ########################\n",
    "        # YOUR CODE HERE\n",
    "        ########################\n",
    "            \n",
    "        \n",
    "        return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eYZX_v8sSC9Q"
   },
   "source": [
    "The next cell will instanciate a tracker and display the evolution of the weights for a single step of tracking. \n",
    "\n",
    "**Question**\n",
    "\n",
    "**Comment** on what you observe, is it following what you're expecting ? What does a high score mean ? What is the expected range of the weights ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlfE4sYGSC9Q",
    "outputId": "e18f8550-65c4-4081-a404-93e3ed8c9345"
   },
   "outputs": [],
   "source": [
    "# Create particle filter\n",
    "pf = ParticleFilterStepTwo(model=model,\n",
    "                           search_space=frame.shape[:2],\n",
    "                           n_particles=200,\n",
    "                           state_dims=2)\n",
    "# Save initial weights\n",
    "w_init = np.copy(pf.weights)\n",
    "\n",
    "# Update tracker using current frame\n",
    "pf.update(frame)\n",
    "\n",
    "# Show updated weights\n",
    "w = pf.weights\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(w_init)\n",
    "plt.plot(w)\n",
    "plt.title('Weights change after `Reweight` step')\n",
    "plt.legend(['Weights initialization',\n",
    "            'Updated weights'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WIC8chCFSC9Q"
   },
   "source": [
    "### 2.3 Step Three : Resample (15 pts)\n",
    "\n",
    "The purpose of this step is to resample a new set of particles using the current estimation of the particle's distribution (*i.e. weights*). Remember, the particles have been previously perturbated and reweighted, therefore the distribution is different. The new set of particles new to be sampled using the updated probability distribution, meaning particles with high probability will be selected more often then the one with small weight.\n",
    "\n",
    "Once a new set of particles have been sampled, you need to be sure that the state is still valid, therefore a clipping step is added in order to ensure the state remain in the image space.\n",
    "\n",
    "**QUESTION**\n",
    "In the following class, implement the resampling scheme inside the function `resample_impl`. \n",
    "\n",
    "*Hint*: You may want to use np.random.choice in your implementation to perform this resampling step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xA02irWFSC9Q"
   },
   "outputs": [],
   "source": [
    "class ParticleFilterStepThree(ParticleFilterStepTwo):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 search_space,\n",
    "                 n_particles,\n",
    "                 state_dims,\n",
    "                 sigma_perturbation=10.0,\n",
    "                 sigma_similarity=20.0,\n",
    "                 alpha=0.0,\n",
    "                 update_rate=10):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param model:         Template image to be tracked (numpy.ndarray)\n",
    "        :param search_space:  Possible size of the search space (i.e. image size)\n",
    "        :param n_particles:   Number of particle used to perform tracking\n",
    "        :param state_dims:    State space dimensions (i.e. number of parameters\n",
    "                              being tracked)\n",
    "        :param sigma_perturbation: How much each particle will be perturbated, float\n",
    "        :param sigma_similarity:  Similarity residual distribution\n",
    "        :param alpha:         Blending factor for model adaptation, float\n",
    "        :param update_rate:   Frequency at which the model will be updated, int\n",
    "        \"\"\"\n",
    "        super(ParticleFilterStepTwo, self).__init__(model,\n",
    "                                                    search_space,\n",
    "                                                    n_particles,\n",
    "                                                    state_dims,\n",
    "                                                    sigma_perturbation,\n",
    "                                                    sigma_similarity,\n",
    "                                                    alpha)\n",
    "        \n",
    "    def update(self, frame):\n",
    "        \"\"\"\n",
    "        Update particle filter for a new frame\n",
    "        :param frame: New frame in the tracking sequence\n",
    "        :return:  Best state (i.e. estimated position\n",
    "        \"\"\"\n",
    "        self.perturbate()\n",
    "        self.reweight(frame)\n",
    "        self.resample()\n",
    "        # Other steps goes here\n",
    "        # ...\n",
    "        \n",
    "        self.state = self.current_state()\n",
    "        return self.state\n",
    "    \n",
    "    def resample_impl(self):\n",
    "        \"\"\" \n",
    "        Resample a new set of particle based on update weight distribution and previously \n",
    "        perturbated particles\n",
    "        Hint: You may want to use `np.random.choice` here\n",
    "        \"\"\"\n",
    "        \n",
    "        ########################\n",
    "        # YOUR CODE HERE\n",
    "        ########################\n",
    "        \n",
    "        \n",
    "        return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ft-HObRiSC9S"
   },
   "source": [
    "The next cell creates a tracker and performs the three steps of the particle filter (*The update step is performed twice in order to emphasis the impact of the resampling step*). It shows the states of the particles at initialization (*i.e. in green*) and after resampling (*i.e. in red*). The last figure shows the expected behaviour.\n",
    "\n",
    "**QUESTION** \n",
    "\n",
    "**Comment** on what you observe, how the particle states change ? Why does it converge to the object ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFKpg78rSC9S",
    "outputId": "d66dc26f-570d-4a5b-9859-8628adb93dca"
   },
   "outputs": [],
   "source": [
    "# Create particle filter\n",
    "pf = ParticleFilterStepThree(model=model,\n",
    "                             search_space=frame.shape[:2],\n",
    "                             n_particles=200,\n",
    "                             sigma_perturbation=20.0,\n",
    "                             sigma_similarity=15.0,\n",
    "                             state_dims=2)\n",
    "# Save initial particle position\n",
    "initial_particles = pf.draw_particles(np.copy(frame))\n",
    "\n",
    "# Update tracker\n",
    "pf.update(frame)\n",
    "pf.update(frame)\n",
    "\n",
    "# Draw particle's state after resampling\n",
    "resampled_particules = pf.draw_particles(np.copy(frame))\n",
    "resampled_true = cv.imread(os.path.join('..','data', 'particles_filter', 'resampled_particles.jpg'))\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(14, 10))\n",
    "display_image(initial_particles, axes=ax[0])\n",
    "ax[0].set_title('Initial particles state')\n",
    "display_image(resampled_particules, axes=ax[1])\n",
    "ax[1].set_title('Resampled particles state')\n",
    "display_image(resampled_true, axes=ax[2])\n",
    "ax[2].set_title('Expected behaviour')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AyN5tYOfSC9S"
   },
   "source": [
    "Now, you will use the `transform_video_file` function you've coded earlier to track Romney's face in the first 300 frames of the video `videos/pres_debate.avi`. The initialized particle filter will pass to the `particles_filter_fn` through the params variable. It will be packed in a dictionnary with the key set to 'tracker'.\n",
    "\n",
    "**QUESTION**\n",
    "\n",
    "**Complete** the implementation of the tracking function. \n",
    "\n",
    "*Hint*: Too draw the state of the tracker, you can use the method `visualize_filter` of the particle filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGfProdySC9T"
   },
   "outputs": [],
   "source": [
    "def particles_filter_fn(frame, params):\n",
    "    \"\"\"\n",
    "    Run one step of particles filter and draw tracking results on top of the image\n",
    "    :param params: dict instance with the following entries: \n",
    "                    'tracker': Tracker instance\n",
    "    :return: frame with detected area\n",
    "    \n",
    "    \"\"\"\n",
    "    res = None\n",
    "    ########################\n",
    "    # YOUR CODE HERE\n",
    "    ########################\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ctn5F1DSC9T"
   },
   "outputs": [],
   "source": [
    "# Load face model\n",
    "model = cv.imread(os.path.join('..', 'data', 'particles_filter', 'face_model.jpg'))\n",
    "# Create particle filter\n",
    "pf = ParticleFilterStepThree(model=model,\n",
    "                             search_space=(720, 1280),\n",
    "                             n_particles=200,\n",
    "                             sigma_perturbation=10.0,\n",
    "                             sigma_similarity=10.0,\n",
    "                             state_dims=2)\n",
    "\n",
    "# Set parameters\n",
    "params = {'tracker': pf}\n",
    "\n",
    "# Call tracking function on video\n",
    "video_path = os.path.join('..', 'data', 'videos', 'pres_debate.avi')\n",
    "results = transform_video_file(video_path, particles_filter_fn, params=params, n_frame=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0isYpiLwSC9T"
   },
   "source": [
    "**QUESTION** \n",
    "\n",
    "In the next cell, display the tracking results every 30 frames.\n",
    "\n",
    "**Comment** on what you observe, is the tracking good ? What happen when he turns his head ? Is the object model good enough ? What happened if you run the detection from scratch several times?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWcW7y69SC9T",
    "outputId": "b447cf44-fa72-496b-88ec-5a9854410d34"
   },
   "outputs": [],
   "source": [
    "\n",
    "########################\n",
    "# YOUR CODE HERE\n",
    "########################\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fk0I_o-USC9U"
   },
   "source": [
    "### 2.4 Step Four : Model adaptation (15 pts)\n",
    "\n",
    "Having a fixed model of the object is not optimal when there is movement (*i.e. out of plane rotation*). Therefore in the next section, a simple model adaptation will be implemented into our particles filter.\n",
    "\n",
    "The model will not be updated every frame to ensure a smooth transition in term of appearance. The update rate is controled with the `update_rate` parameter in the constructor. The model adaptation consists of a linear interpolation between the patch located at the current best position estimation (*i.e. self.state*) and the previous model.\n",
    "\n",
    "$$\n",
    "M_{new} = \\alpha \\cdot P_{best} + \\left(1 - \\alpha \\right) M_{old}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ control the amount of blending between the two patches, $P_{best}$ is the patch around the current estimation of the object position and $M_{old}$ is the actual model of the object. \n",
    "\n",
    "**QUESTION**\n",
    "\n",
    "In the next cell, implement the model adaptation in the method `update_model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PaOug7qSC9U"
   },
   "outputs": [],
   "source": [
    "class ParticleFilterStepFour(ParticleFilterStepThree):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 search_space,\n",
    "                 n_particles,\n",
    "                 state_dims,\n",
    "                 sigma_perturbation=10.0,\n",
    "                 sigma_similarity=20.0,\n",
    "                 alpha=0.0,\n",
    "                 update_rate=10):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param model:         Template image to be tracked (numpy.ndarray)\n",
    "        :param search_space:  Possible size of the search space (i.e. image size)\n",
    "        :param n_particles:   Number of particle used to perform tracking\n",
    "        :param state_dims:    State space dimensions (i.e. number of parameters\n",
    "                              being tracked)\n",
    "        :param sigma_perturbation: How much each particle will be perturbated, float\n",
    "        :param sigma_similarity:  Similarity residual distribution\n",
    "        :param alpha:         Blending factor for model adaptation, float\n",
    "        :param update_rate:   Frequency at which the model will be updated, int\n",
    "        \"\"\"\n",
    "        super(ParticleFilterStepTwo, self).__init__(model,\n",
    "                                                    search_space,\n",
    "                                                    n_particles,\n",
    "                                                    state_dims,\n",
    "                                                    sigma_perturbation,\n",
    "                                                    sigma_similarity,\n",
    "                                                    alpha)\n",
    "        \n",
    "    def update(self, frame):\n",
    "        \"\"\"\n",
    "        Update particle filter for a new frame\n",
    "        :param frame: New frame in the tracking sequence\n",
    "        :return:  Best state (i.e. estimated position\n",
    "        \"\"\"\n",
    "        self.perturbate()\n",
    "        self.reweight(frame)\n",
    "        self.resample()\n",
    "        self.state = self.current_state()\n",
    "        self.frame_counter += 1\n",
    "        if self.alpha > 0.0 and (self.frame_counter % self.update_rate) == 0:\n",
    "            self.update_model(frame)\n",
    "        return self.state\n",
    "    \n",
    "    def update_model(self, frame):\n",
    "        \"\"\"\n",
    "        This function perform a `model update using the current best estimation of\n",
    "        the object position (i.e. state) and linearly blend the previous model and\n",
    "        the patch at the best state:\n",
    "          model_new = alpha * best_patch + (1 - alpha) * model_old\n",
    "        It also perform so sanity check to ensure the dimensions of the model are\n",
    "        valid.\n",
    "        :param frame: Current frame\n",
    "        \"\"\"\n",
    "        \n",
    "        ###############\n",
    "        # YOUR CODE HERE\n",
    "        ###############\n",
    "\n",
    "        return "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9txQVxVCSC9U"
   },
   "source": [
    "**QUESTION** \n",
    "\n",
    "Now that you have a working object tracker, apply the same methodology to track Romney's left hand and again display every frames that are a multiple of 30. You can use the following patch as model *x=520, y=375, w=104, h=129*.\n",
    "\n",
    "Reuse the same structure has what we've done when tracking his face.\n",
    "\n",
    "**Discuss** the influence of the various parameters (i.e. Number of particles, sigma_perturbation, sigma_similarity, alpha, update_rate, ...) on the performance of the tracking. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0uQvRVvSC9U",
    "outputId": "0651e8a0-ef08-4224-f3f1-33cf4ce1d8e3"
   },
   "outputs": [],
   "source": [
    "\n",
    "###############\n",
    "# YOUR CODE HERE\n",
    "###############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nc1kvLCXSC9V"
   },
   "source": [
    "## 3 Face Tracking\n",
    "\n",
    "Up to now, the frames of the video were considered as single image having no relationship between them. However, this is not the best approach: the correlation between two consecutive frames is large, since the image will displace or change in the next frame with respect to the previous one only by few pixels. We can rely on this to build a smarter detector. \n",
    "\n",
    "The case study for the next sections will be face tracking. The task is to provide a bounding box where the face is located. A number of methods will be analysed and benchmarked against each other. The baseline will be established with a standard face detector based on [Viola, Jones](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf) work (*i.e. no tracking*). Then, you will compare it against the tracking algorithms.\n",
    "\n",
    "The methods to use and compare are the following:\n",
    "\n",
    "- Frame-wise face detection (Baseline)\n",
    "- Tracking: 'MIL', 'KCF', 'TLD', 'MedianFlow', 'Mosse'\n",
    "\n",
    "For comparison, you will be asked  to implemented the following metrics:\n",
    "\n",
    "- Euclidean distance between center's bounding box\n",
    "- Intersection over Union\n",
    "- Computation time\n",
    "\n",
    "Your first task is to implement the preprocessing function that will be applied on every frame before actually doing the tracking. You can apply any transformation you want to each frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXGUDFx1SC9V",
    "outputId": "48d808a9-7925-4a01-ca54-a682b0fac105"
   },
   "outputs": [],
   "source": [
    "# Load all images\n",
    "def preprocessing_fn(image, params):\n",
    "    \"\"\"\n",
    "    Apply preprocessing on a given image.\n",
    "    \n",
    "    :param image: Image to preprocess\n",
    "    :param params: Extra parameters\n",
    "    :return: Preprocessed image\n",
    "    \"\"\"\n",
    "    # Indentity function, but you can do whatever you need to\n",
    "    return image\n",
    "\n",
    "file_path =  os.path.join('..','data', 'videos', 'speaker.avi')\n",
    "images = transform_video_file(file_path, preprocessing_fn, n_frame=350)\n",
    "print('Video contains a total of {} frames'.format(len(images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd7TSNglSC9W"
   },
   "source": [
    "### 3.1 Face Detection (1pt)\n",
    "\n",
    "The detection baseline is established using an instance of `cv::CascadeClassifier` which implements the approach proposed by Viola & Jones for object detection. \n",
    "\n",
    "Your task is to complete the implementation of the `detection_fn` which, given an instance of a classifier and an image, return the bounding box where the face is located. More information about the detector can be found in the [doc](https://docs.opencv.org/3.4.4/d1/de5/classcv_1_1CascadeClassifier.html#ab3e572643114c43b21074df48c565a27).\n",
    "\n",
    "Be sure to implement a solution to deal with the situation where multiple boxes are returned by the classifier. The function must return **ONLY** one bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEImP3xXSC9W"
   },
   "outputs": [],
   "source": [
    "def detection_fn(detector, image):\n",
    "    \"\"\"\n",
    "    Run face detection on a given `image` with an instance of CascadeClassifier\n",
    "    \n",
    "    :param detector: CascadeClassifier instance\n",
    "    :param image: Frame on which to run detection\n",
    "    :return: bounding box if any or None\n",
    "    \"\"\"\n",
    "    bbox= None\n",
    "    #######################################\n",
    "    # YOUR CODE HERE\n",
    "    #######################################\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5IXZafHSC9X"
   },
   "source": [
    "The code in the following cell will test your `detection_fn` and display the outcome for the first four frames of the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XaiYt6oMSC9X",
    "outputId": "97aec30c-70cc-41d1-9f89-74076daac65b"
   },
   "outputs": [],
   "source": [
    "# Load face detector\n",
    "fpath = os.path.join('..', 'data', 'haarcascade_frontalface_alt2.xml')\n",
    "fdet = cv.CascadeClassifier(fpath)\n",
    "assert fdet.empty() is not True\n",
    "\n",
    "# Detect \n",
    "detection_out= []\n",
    "for img in images[:4]:\n",
    "    bbox = detection_fn(fdet, img)\n",
    "    detection_out.append(bbox)\n",
    "    \n",
    "# Display\n",
    "fig, ax = plt.subplots(2, 2, figsize=(17, 9))\n",
    "for k, box in enumerate(detection_out):\n",
    "    # Draw rectangle\n",
    "    x, y, w, h = box\n",
    "    im = cv.rectangle(images[k].copy(), (x, y), (x + w -1, y + h -1), (255, 0, 0), 3, lineType=cv.LINE_AA)\n",
    "    # Display\n",
    "    r = k // 2\n",
    "    c = k % 2\n",
    "    display_image(im, axes=ax[r][c])\n",
    "    ax[r][c].set_title('Detection at frame {}'.format(k))\n",
    "    ax[r][c].set_xticks([])\n",
    "    ax[r][c].set_yticks([]);    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E78C7y_OSC9X"
   },
   "source": [
    "### 3.2 Face Tracking (2pts)\n",
    "\n",
    "Similar to what you have done before, you will have to complete the `tracking_fn` that will perform the tracking step. Given one instance of `cv::Tracker` [(doc)](https://docs.opencv.org/3.4.4/d0/d0a/classcv_1_1Tracker.html) and a  input image, the function will return the bounding box where the object (*i.e. face*) is located.\n",
    "\n",
    "Again, be sure to **ONLY** one bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9gf_TYbSC9Y"
   },
   "outputs": [],
   "source": [
    "def tracking_fn(tracker, image):\n",
    "    \"\"\"\n",
    "    Perform tracking on a given image.\n",
    "    \n",
    "    :param tracker: Tracker instance\n",
    "    :param image: Image on which to run tracker\n",
    "    :return: Bounding box if any\n",
    "    \"\"\"\n",
    "    bbox = None\n",
    "    #######################################\n",
    "    # YOUR CODE HERE\n",
    "    #######################################\n",
    "    return bbox "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Of760xcbSC9Y"
   },
   "source": [
    "The following cell will test your `tracking_fn` in the same way as what we did earlier b traking the first four frames of the video and displaying the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Rr7sZgJSC9Y",
    "outputId": "3b671787-70fb-4203-9fce-11d418d205f5"
   },
   "outputs": [],
   "source": [
    "# Create tracker\n",
    "tracker = create_face_tracker(name='KCF')\n",
    "tracker.init(images[0], (551, 121, 153, 153))\n",
    "\n",
    "# Detect \n",
    "tracking_out=[]\n",
    "for img in images[:4]:\n",
    "    bbox = tracking_fn(tracker, img)\n",
    "    tracking_out.append(bbox)\n",
    "           \n",
    "# Display\n",
    "fig, ax = plt.subplots(2, 2, figsize=(17, 9))\n",
    "for k, box in enumerate(tracking_out):\n",
    "    # Draw rectangle\n",
    "    x, y, w, h = box\n",
    "    x = int(x); y=int(y); w=int(w); h=int(h)\n",
    "    im = cv.rectangle(images[k].copy(), (x, y), (x + w -1, y + h -1), (0, 255, 0), 3, lineType=cv.LINE_AA)\n",
    "    # Display\n",
    "    r = k // 2\n",
    "    c = k % 2\n",
    "    display_image(im, axes=ax[r][c])\n",
    "    ax[r][c].set_title('Tracking at frame {}'.format(k))\n",
    "    ax[r][c].set_xticks([])\n",
    "    ax[r][c].set_yticks([]);    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NP81r1X-SC9Z"
   },
   "source": [
    "### 3.3 Comparison\n",
    "\n",
    "#### 3.4.1 Metric\n",
    "\n",
    "The performance of a given tracker will be assessed with the following metrics:\n",
    "\n",
    "- Intersection over Union\n",
    "- Distance between center's bounding boxes\n",
    "- Execution time\n",
    "\n",
    "##### 3.4.1.1 Intersection over Union (1pt)\n",
    "\n",
    "Your task is to complete the implementation of the `iou_metric` function. The function will return the *Intersection over Union* given two bounding boxes, namely `box_a` and `box_b`.\n",
    "\n",
    "You can find the metric as the Jaccard index https://en.wikipedia.org/wiki/Jaccard_index. Feel free to modify the implementation below to adapt it to your function output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iWU9kcKSC9Z"
   },
   "outputs": [],
   "source": [
    "def iou_metric(box_a, box_b):\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IoU) between two bounding boxes (x, y, w, h)\n",
    "    \n",
    "    :param box_a: First bounding box\n",
    "    :param box_b: Second bounding box\n",
    "    :return: Intersction over Union\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    iou = 0.0\n",
    "    #######################################\n",
    "    # YOUR CODE HERE\n",
    "    #######################################\n",
    "    \n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nOYakHcSC9Z",
    "outputId": "ba140d59-5a8e-49fa-8adf-dd6626428b83"
   },
   "outputs": [],
   "source": [
    "# Usage + sanity check\n",
    "IoU = iou_metric(box_a=[39, 63, 164, 49], box_b=[40, 63, 165, 49])\n",
    "print('Intersection over Union: {:.3f}'.format(IoU))\n",
    "assert round(IoU, 3) == 0.982, 'Somehting went wrong at your implementation of the `iou_metric` function'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rdldw_uYSC9a"
   },
   "source": [
    "##### 3.4.1.2 Center's distance (1pts)\n",
    "\n",
    "Complete the `center_metric` function that compute the euclidean distance between two bounding box centers, define as:\n",
    "\n",
    "$$\n",
    "d_i = \\left|\\left| c_i^A - c_i^B \\right|\\right|\n",
    "$$\n",
    "\n",
    "where `A` and `B` are the two bounding boxes (*i.e. detection + ground truth*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhJbTSMaSC9a"
   },
   "outputs": [],
   "source": [
    "def center_metric(box_a, box_b):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between two given bounding boxes\n",
    "    \n",
    "    :param box_a: First bounding box\n",
    "    :param box_b: Second bounding box\n",
    "    :return: Distance\n",
    "    \"\"\"\n",
    "    \n",
    "    d = 0.0\n",
    "    #######################################\n",
    "    # YOUR CODE HERE\n",
    "    #######################################\n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyETHz9FSC9a",
    "outputId": "afb6b95b-573e-4127-e3ac-f270fd94d692"
   },
   "outputs": [],
   "source": [
    "# Usage + sanity check\n",
    "dist = center_metric(box_a=[39, 63, 164, 49], box_b=[40, 63, 165, 49])\n",
    "print('Distance: {:.3f}'.format(dist))\n",
    "assert round(dist, 3) == 1.500, 'Somehting went wrong at your implementation of the `iou_metric` function'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hqYhBgvSC9a"
   },
   "source": [
    "##### 3.4.1.3 Execution time (1pt)\n",
    "\n",
    "The function `time_metric` will measure the execution time of a given function defined in `processing_fn`. Its signature must be `processing_fn() -> Any`.\n",
    "If the `processing_fn` return some values, they will be passed through the `time_metric` function. Therefore the return value is a tuple containing the execution time in first position and anything returned by `processing_fn` in the second position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqQQ_3pBSC9a"
   },
   "outputs": [],
   "source": [
    "def time_metric(processing_fn):\n",
    "    \"\"\"\n",
    "    Measure the execution time of a given function (i.e. lambda function)\n",
    "    \n",
    "    :param processing_fn:  Lambda function to be timed\n",
    "    :return: Execution time in milli-seconds\n",
    "    \"\"\"\n",
    "    ex_time = 0\n",
    "    retval = None\n",
    "    #######################################\n",
    "    # YOUR CODE HERE\n",
    "    #######################################\n",
    "    \n",
    "    return ex_time, retval\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79VfBOLCSC9b"
   },
   "source": [
    "The `time_metric` function can be used as follow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzLCp8fFSC9b",
    "outputId": "4874b17a-0241-42da-f9d1-2d456c94c6c7"
   },
   "outputs": [],
   "source": [
    "# Using user define function with proper prototype\n",
    "def wasting_time_func():\n",
    "    _time.sleep(0.75)\n",
    "    return 'Done'\n",
    "    \n",
    "# Call metric\n",
    "dt, ret = time_metric(wasting_time_func)\n",
    "print('User-defined function took: {:.3f} ms, with return value {}'.format(dt, ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRw4MpRZSC9b",
    "outputId": "129e35d9-04a8-4bf3-b81a-22293f6c6b16"
   },
   "outputs": [],
   "source": [
    "# Using lambda function \n",
    "dt, ret = time_metric(lambda: _time.sleep(0.5))\n",
    "print('Lambda function took: {:.3f} ms'.format(dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMD9Tz7nSC9b"
   },
   "source": [
    "#### 3.4.2 Load Ground Truth (2pts)\n",
    "\n",
    "The true bounding boxes are stored into the `data/videos/speaker_gt.txt` text file. Each line contains the true bounding box for the corresponding frame.\n",
    "\n",
    "Your task is to complete the `load_ground_truth` that parses the file and return a list of boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyiDikXgSC9b"
   },
   "outputs": [],
   "source": [
    "def load_ground_truth(filename):\n",
    "    \"\"\"\n",
    "    Load all detection stored into a given file\n",
    "    \n",
    "    :param filename: Path to the text file storing the ground truth\n",
    "    :return: List of bounding boxes\n",
    "    \"\"\"\n",
    "    bboxes = []\n",
    "    ###########################\n",
    "    # YOUR CODE HERE\n",
    "    ###########################\n",
    "    \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXeNBl__SC9d"
   },
   "source": [
    "Load every face bounding boxes into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJQc43HRSC9d",
    "outputId": "0e5055fa-f267-4f0e-9e1a-083b0a5bd115"
   },
   "outputs": [],
   "source": [
    "# Load ground truth\n",
    "fname = os.path.join('..', 'data', 'videos', 'speaker_gt.txt')\n",
    "gt_bbox = load_ground_truth(filename=fname)\n",
    "print('There is a total of {} bounding boxes'.format(len(gt_bbox)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMjp7BxKSC9d"
   },
   "source": [
    "\n",
    "#### 3.4.3 Experimental setup (2pts)\n",
    "\n",
    "In the `run_experiment` function, given a list of pair `{Image, Ground truth}`, it will perform the tracking/detection on all images and run the various *metrics* that have been implemented earlier. \n",
    "\n",
    "The results for a **single** frame will be stored into a dictionary with the following entries:\n",
    "\n",
    "- `bbox` will contain the detected region\n",
    "- `metrics` will contain all the metrics computed during the experiment. The metrics are stored in a `tuple` ordered as : `IoU, Distance, Time`\n",
    "\n",
    "\n",
    "The next cells run the experiment with the `detection_fn` in order to establish the baseline. Later, on the tracking results, all the metrics will be compared against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIfAA4_uSC9d"
   },
   "outputs": [],
   "source": [
    "def run_experiment(images, true_bbox, tracking_fn):\n",
    "    \"\"\"\n",
    "    Run a given detection/tracking function on a list of images.\n",
    "    \n",
    "    :param images: List of consecutive images to processed (already preprocessed)\n",
    "    :param bboxes: List of true bounding boxes (Ground truth)\n",
    "    :param tracking_fn: Function running detection/tracking for ONE frame\n",
    "    :return: List of dictionnaries for each frames holding detected bounding box if any and the various metrics\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    for img, gt_bbox in zip(images, true_bbox):\n",
    "        # Run metrics + tracking\n",
    "        dt, bbox = time_metric(lambda: tracking_fn(img))        \n",
    "        iou = iou_metric(bbox, gt_bbox)\n",
    "        dist = center_metric(bbox, gt_bbox)\n",
    "        # Accumulate results\n",
    "        metrics.append({'bbox': bbox, 'metric': (iou, dist, dt)})\n",
    "    # Done\n",
    "    return metrics\n",
    "\n",
    "# Run detection, no tracking\n",
    "detection_exp = run_experiment(images=images, true_bbox=gt_bbox, tracking_fn=lambda x: detection_fn(fdet, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qB2DRRcZSC9d"
   },
   "source": [
    "Your task is to run the same experiment (*i.e. `run_experiment`*) for all the selected trackers. To instantiate the tracker using its name, you have to use the function `create_face_tracker(str) -> cv::Tracker`.\n",
    "\n",
    "For the initialisation of the tracker, you can use the region: `(551, 121, 175, 175)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjnCX7PiSC9d"
   },
   "outputs": [],
   "source": [
    "# List of tracker to use\n",
    "trackers_name = ['MIL', 'KCF', 'TLD', 'MedianFlow', 'Mosse']\n",
    "# Run experiments for each tracker\n",
    "trackers_exp = {}\n",
    "for name in trackers_name:\n",
    "    \n",
    "    ###########################\n",
    "    # YOUR CODE HERE\n",
    "    ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHO0WoM8SC9d"
   },
   "source": [
    "#### 3.4.4 Results (15pts)\n",
    "\n",
    "In this section we ask you to display and discuss about the outcome of the tracking experiments. Compare every tracker with the baseline established earlier and discuss what you observed.\n",
    "\n",
    "Report the following quantities:\n",
    "\n",
    "- For each metric, plot its value for each frame for each method on the same graph. You should have three plots.\n",
    "- The `min`, `max`, `mean`, `std` of each metrics for every tracker and the baseline, comment on the values\n",
    "- Display the detection/tracking of the `best` and the `worst` IoU for each algorithm\n",
    "- Discuss the performance of each tracker\n",
    "- Discuss the pros and cons of each metrics\n",
    "- Investigate each tracker and explain briefly what is the methods used under the hood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWVUJG7PSC9d",
    "outputId": "18babf7e-0470-4f4e-f7b8-58b4eb7e1873"
   },
   "outputs": [],
   "source": [
    "# Add detection restults\n",
    "trackers_exp['Det'] = detection_exp\n",
    "\n",
    "str_info    = 'Metric {} | Mean: {:.2f} | Min: {:.2f} | Max: {:.2f} | Std: {:.2f}'\n",
    "\n",
    "print(\"\\n****** IoU ******\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "for key, value in trackers_exp.items():\n",
    "    IoU = [k['metric'][0] for k in value]\n",
    "    plt.plot(IoU)\n",
    "    print(str_info.format(key, np.mean(IoU), np.min(IoU), np.max(IoU), np.std(IoU)))\n",
    "plt.legend(list(trackers_exp.keys()))\n",
    "plt.title('Intersection over Union') \n",
    "\n",
    "\n",
    "print(\"\\n****** Distance ******\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "for key, value in trackers_exp.items():\n",
    "    dist = [k['metric'][1] for k in value]\n",
    "    plt.plot(dist) \n",
    "    print(str_info.format(key, np.mean(dist), np.min(dist), np.max(dist), np.std(dist)))\n",
    "\n",
    "plt.legend(list(trackers_exp.keys()))\n",
    "plt.title('Center\\'s distance')\n",
    "\n",
    "\n",
    "print(\"\\n****** Processing time ******\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "for key, value in trackers_exp.items():\n",
    "    dt = [k['metric'][2] for k in value]\n",
    "    plt.plot(dt)    \n",
    "    print(str_info.format(key, np.mean(dt), np.min(dt), np.max(dt), np.std(dt)))\n",
    "\n",
    "plt.legend(list(trackers_exp.keys()))\n",
    "plt.title('Processing time [ms]')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "pQR4zDNiSC9N",
    "jmY6DCP_SC9P",
    "WIC8chCFSC9Q",
    "fk0I_o-USC9U",
    "bd7TSNglSC9W",
    "E78C7y_OSC9X",
    "NP81r1X-SC9Z",
    "XMD9Tz7nSC9b",
    "DMjp7BxKSC9d",
    "GHO0WoM8SC9d"
   ],
   "name": "03_object_tracking_solution.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "907933357ae3fbb9d3056f41106fcc50cd829035cc4c8630c07b8c7aaf7c597e"
  },
  "kernelspec": {
   "display_name": "IPLabEnv",
   "language": "python",
   "name": "iplabenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
